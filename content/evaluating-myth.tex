In \autoref{ch:implementation}, we developed an efficient synthesis procedure from our core synthesis calculus \mlsyn{}.
In this chapter, we explore our implementation of this synthesis procedure, a prototype program synthesizer called \myth{}.\footnote{%
  We provide a summary of the final synthesis procedure as a reference in \autoref{app:the-implementation-of-myth}.
}

Our goal with \myth{} is to further explore the type-theoretic foundations for program synthesis that we have developed so far.
We started our exploration by carefully analyzing the metatheory of type-directed program synthesis, in particular the soundness and completeness of \lsyn{} and \mlsyn{}.
However, this is insufficient for getting a complete sense of how program synthesis systems built up top of these foundations will perform in practice.
By exploring the behavior of an actual implementation, we can better understand the capabilities and limitations of our approach and identify areas for future improvement.

Note that an explicit non-goal of this exploration is to justify \myth{}-the-artifact as a practical tool for program synthesis.
While we explore some aspects of the viability of \myth{} as an end-user tool, \eg, performance, we intentionally do not explore the usability of the tool.
We do this primarily as a matter of pragmatics.
There are plenty of empirical questions to investigate about \myth{}---How many examples do we need to synthesize a particular program?  How long does it take to synthesize a particular program?---without delving into the usability side of the project.
However, we also want to stress that, throughout this work, we have been less concerned with building a practical tool and more interested in answering foundational questions about the integration of types into program synthesis.
We do not want to overshadow these important results with claims about usability that we do not have the time to develop thoroughly.
We leave such investigation to future work.

\section{Search Parameter Tuning}
\label{sec:search-parameter-tuning}

Originally we started with a simple synthesis procedure that searched the space of programs according to program size in a breadth-first manner.
When we introduced refinement trees, we also introduced a number of search parameters into the procedure:
\begin{itemize}
  \item $s$, the maximum scrutinee size of any match expression,
  \item $m$, the maximum match depth, \ie, the maximum number of matches that can appear in any branch of a refinement tree, and
  \item $k$, the maximum size of $E$-terms that we generate during the $E$-guess phase of the procedure.
\end{itemize}
Rather than performing a breadth-first search by program size alone, we now perform a breadth-first search according to these three parameters.
Thus, choosing appropriate \emph{initial values} along with a \emph{strategy} for traversing through successive iterations of the algorithm is imperative towards obtaining good synthesis results.

\paragraph{Scrutinee Size}
Let's consider the effects of each of these parameters on the programs that the synthesis algorithm produces.
First, the scrutinee size affects the complexity of the pattern matches that we can synthesize.
At $s = 1$, we can only synthesize variables as scrutinees.
This has been sufficient for the examples we have used so far, but in general we need the ability to synthesize richer scrutinees.
As an example, suppose in OCaml that we have the standard $\mkwd{lookup}$ function of type $'a → ('a, 'b)\;\mlist → 'b\;\mkwd{option}$ in our context.
Then we need richer scrutinees to synthesize programs that use association lists such as:

\begin{center}
  \begin{minipage}{0.75\textwidth}
    \begin{lstlisting}
      match l with
      | Some r -> (* Process the result *)
      | None   -> (* Return some default value *)
    \end{lstlisting}
  \end{minipage}
\end{center}

However, we want to avoid opening the door to complex scrutinees too quickly.
This is because the number of possible scrutinees (and consequently, the number of possible matches) grows exponentially with the scrutinee size and very often, we only need to pattern match over a single variable.
Our informativeness restriction rules out some of these scrutinees, but in practice, many scrutinees make it through the restriction, requiring us to add these branches to our refinement tree.
These additional branches become more points where we have to $E$-guess, making synthesis more costly.
Furthermore, on top of these scrutinees, as we increase the scrutinee size $s$, we also increase the liklihood of generating scrutinees that are equivalent to previously generated scrutinees.

\paragraph{Match Depth}
Related to the scrutinee size, the match depth $m$ controls the number of match expressions that can appear in any branch of the refinement tree.
Whereas the scrutinee size controls the \emph{width} of matches, \ie, the amount of possible match expressions at any point in a refinement tree, the match depth controls the \emph{depth} of matches, \ie, how deeply we can nest pattern matches in a candidate program.
In many cases, a single match expression is sufficient to satisfy a given synthesis problem.
However, sometimes we may also need to resort to nested matching to access successive elements of a structure or to make decisions based off of case analysis of multiple pieces of data.
The danger is that unnecessary nested matches greatly impact performance because in addition to the branching factor, each branch introduces additional binders which accelerates the exponential blow-up associated with raw-term enumeration.
Thankfully, we rule out equivalent, nested matches thanks to our informativeness restriction (see \autoref{subsec:reigning-in-matches}), although the cost of stacking up binders due to nested matches is very significant.

An additional problem with setting the match depth too high is overspecialization.
For example, consider the overspecializing function for $\mkwd{stutter}$ from \autoref{subsec:the-minimum-program-principle}.
\[
  \begin{array}{l}
    \mfix\;f\;(l{:}\mlist) : \mlist = \\
    \⇥{1} \mmatch\;l\;\mwith \\
    \⇥{1}   \bnfalt \mNil → [] \\
    \⇥{1}   \bnfalt \mCons(x, l') → \mmatch\;l'\;\mwith \\
    \⇥{2}   \bnfalt \mNil → [0, 0] \\
    \⇥{2}   \bnfalt \mCons(y, l'') → \mmatch\;l''\;\mwith \\
    \⇥{3}   \bnfalt \mNil → [1, 1, 0, 0] \\
    \⇥{3}   \bnfalt \mCons(z, l''') → [].
  \end{array}
\]
By synthesizing programs in order of increasing size, we guaranteed that our original synthesis procedure would not synthesize this program before synthesizing the desired recursive function.
However, imagine in our new synthesis procedure that we started with the match depth $m = 3$.
Notice that because overspecialization contains no $E$-guessed terms, our initial refinement tree would contain this program immediately and we would be done!
Because of this, we have to start with a small match depth and be careful about increasing it too quickly.

\paragraph{$E$-term Size}
Finally, $k$ controls the maximum size of $E$-terms that the algorithm is allowed to guess at any node in the refinement tree.
The primary benefit of our refinement tree structure, is that we have localized the expensive procedure of $E$-term enumeration to the leaves of the refinement tree.
This allows us to generate much smaller $E$-terms on average.
For example, in our original algorithm, when we synthesized terms up to size 11, we would need to explore the space of $E$-terms of size 11 if our context allowed us to generate $E$-terms of that size.
In our updated algorithm, rather than having to synthesize $E$-terms up to size 11 to find $\mkwd{stutter}$, we only need to synthesize $E$-terms up to size 3 to find the recursive function call $f\,l'$.
This is a tremendous win in terms of efficiency because of the exponential cost of enumerating large $E$-terms.

Unlike match depth, we do not run into issues of overspecialization if $k$ begins too large.
In fact, by admitting larger $E$-terms, we use more variables and function applications which, by our Minimum Programming Principle (\autoref{subsec:the-minimum-program-principle}) makes it more likely our synthesized program generalizes to the behavior that the user intends.
Furthermore, the cost of synthesizing $E$-terms of modest size is very small.
It is not until we reach the critical point in the exponential cliff that generation time jumps from sub-seconds to minutes.
So we are safe in starting with an $E$-term size of reasonable size.

\subsection{Search Strategy}

With these considerations in mind, we can now discuss the particular search strategy we implemented in \myth{}.
With multiple search parameters, we could devise an \emph{adaptive} strategy that responds to the performance of the synthesizer for the particular synthesize problem at hand.
For example, we may vary the initial $E$-term size $k$ or how much we increment the $k$ depending on the size of the context we synthesize under.
Because our context grows as we move down in the refinement tree as we add binders with functions or matches, we would need to vary the search parameters at different points in the tree.

For these experiments, we opted for a simpler \emph{static} strategy suitable for the domain of programs we intended to synthesize---simple functional programs over recursive data types such as lists and trees.  We begin with the following initial search parameter values:
\begin{itemize}
  \item The maximum scrutinee size, $s = 1$.
  \item The maximum match depth, $m = 0$.
  \item The maximum $E$-guess size, $k = 13$.
\end{itemize}
We then move through the following phases of $I$-refinement.
In each phase of $I$-refinement, we increase one of our search parameters and then extend the refinement tree accordingly.
In particular, when we increase the match depth or scrutinee size, we traverse the refinement tree, create new match expressions as necessary, and carry out further $I$-refinements (in particular, application of \rulename{irefine-arr} and \rulename{irefine-base}) where ever possible.
Finally, after each $I$-refinement phase, we allocate 0.25 seconds to generates $E$-terms up to the size limit $k$ at each eligible refinement tree node and then propagate the results to see if we can synthesize a complete, satisfying program.

The phases of $I$-refinement we go through are:
\begin{enumerate}
  \item Create an initial refinement tree with the search parameters $s$ and $m$.
  \item Increase $m$ by one ($m = 1$) and extend the refinement tree accordingly.
  \item Increase $m$ by one ($m = 2$) and extend the refinement tree accordingly.
  \item Increase $s$ by five ($s = 6$) and extend the refinement tree accordingly.
  \item Increase $m$ by one ($m = 3$).
\end{enumerate}
By the end of the process, we synthesize programs with $E$-terms of size 13, triply-nested pattern matches, with scrutinees up to size six.
This is simple strategy is sufficient to handle all of the examples that we discuss in \autoref{sec:benchmark-suite}.

\section{Example Development}
\label{sec:example-development}

\input{figures/myth-stutter}

The input to the \myth{} synthesizer is a series of top-level declarations in the subset of OCaml identified by \mlsyn{} along with a synthesis problem---a name, goal type, and example values.
For example, \autoref{fig:myth-stutter} gives the source code for the $\mkwd{stutter}$ in \myth{} that we have studied extensively in the previous chapters.
\myth{} has no built-in data types, so we must provide them all ourselves using algebraic data types.
Here, we declare a type of monomorphic $\mlist$s whose carrier type is $\mnat$.
The declaration of $\mkwd{stutter}$ contains the familiar three input/output examples\footnote{%
  For sake of convenience, \myth{} provides syntactic sugar for representing lists and natural numbers.
}
suggesting the intended behavior of $\mkwd{sutter}$.

We arrived at this set of examples by using the following process:
\begin{enumerate}
  \item We observed that the likely data type we need to perform induction over was $\mlist$, so we provided examples of how the function should work in both cases of $\mlist$ as defined its constructors: $[] ⇒ [] \bnfalt [0] ⇒ [0; 0]$.
  \item We ran \myth{} with these examples, obtaining the program:

    \begin{minipage}{0.5\textwidth}
      \begin{center}
        \begin{lstlisting}
          let list_stutter : list -> list =
            fun (l1:list) -> match l1 with
                               | Nil -> []
                               | Cons (n1, l2) -> [0; 0]
          ;;
        \end{lstlisting}
      \end{center}
    \end{minipage}

    Upon inspection, this program is not sufficient because while it has the correct behavior in the $\mNil$ case, it does not have the correct behavior in the $\mCons$ case, returning a constant list rather than performing a more general calculation.

  \item To fix this problem, we need to add additional examples to rule out this expression in the $\mCons$ branch.
    Keeping in mind our trace completeness restriction to power recursive function calls (\autoref{subsec:trace-completeness}), we add
    one additional example that is trace complete with respect to the original examples, $[1; 0] ⇒ [1; 1; 0; 0]$, and see if that creates a satisfactory program.
    Updating our program with this additional example and running it through \myth{} yields:

    \begin{minipage}{0.6\textwidth}
      \begin{center}
        \begin{lstlisting}
          let list_stutter : list -> list =
            let rec f1 (l1:list) : list =
              match l1 with
                | Nil -> []
                | Cons (n1, l2) -> Cons (n1, Cons (n1, f1 l2))
            in
              f1
          ;;
        \end{lstlisting}
      \end{center}
    \end{minipage}

    Which is the standard implementation of $\mkwd{stutter}$, so we are done!
\end{enumerate}

We use this process of iterative example refinement based on the inductive structure of the data type to develop all of the programs we supply to \myth{}.

\paragraph{Multi-argument Functions}

When synthesizing single argument functions, we can perform case analysis using our examples as described above in a straight forward manner.
However, the process becomes more complex with multi-argument functions.
In general, we will need additional examples demonstrating how our function behaves with each argument.

As a simple example, consider synthesizing the $\mkwd{append}$ function of type $\mlist → \mlist → \mlist$.
Since the only argument type is $\mlist$, it makes sense that we ought to perform case analysis on one of the $\mlist$ arguments.
Let's choose the first argument and try the following examples:
\begin{align*}
 & [] ⇒ [] ⇒ [] \\
\bnfalt & [0] ⇒ [] ⇒ [0] \\
\bnfalt & [1,0] ⇒ [] ⇒ [1,0]
\end{align*}
Feeding these examples to \myth{} produces the following function

\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{lstlisting}
      let list_append : list -> list -> list =
        fun (l1:list) -> fun (l2:list) -> l1
      ;;
    \end{lstlisting}
  \end{center}
\end{minipage}

This is not the $\mkwd{append}$ function we want; it is the function that always chooses its first argument!
Of course, looking at the examples we provided, this is certainly the simplest program that satisfies the examples.

The problem is that we have not specified the behavior of $\mkwd{append}$ on non-trivial second arguments.
Let's try doing so incrementally, first introducing a single extra example:
\begin{align*}
 & [] ⇒ ([] ⇒ [] \bnfalt [0] ⇒ [0]) \\
\bnfalt & [0] ⇒ [] ⇒ [0] \\
\bnfalt & [1,0] ⇒ [] ⇒ [1,0]
\end{align*}
Running \myth{} on these examples yields the refined function:

\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{lstlisting}
      let list_append : list -> list -> list =
        fun (l1:list) ->
          fun (l2:list) -> match l2 with
                             | Nil -> l1
                             | Cons (n1, l3) -> Cons (0, l1)
      ;;
    \end{lstlisting}
  \end{center}
\end{minipage}

This function is closer to $\mkwd{append}$ but still not there.
In particular, it only append a $0$ onto $l1$ in the $\mCons$ case, completely ignoring $l2$ in the process.
Adding another example to handle when we call $\mkwd{append}\,[0]\,[0]$ results in the same program.
We must add one additional example,
\begin{align*}
 & [] ⇒ ([] ⇒ [] \bnfalt [0] ⇒ [0]) \\
\bnfalt & [0] ⇒ ([] ⇒ [0] \bnfalt [0] ⇒ [0, 0]) \\
\bnfalt & [1,0] ⇒ ([] ⇒ [1, 0] \bnfalt [0] ⇒ [1, 0, 0]).
\end{align*}
On these examples \myth{} produces the program

\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{lstlisting}
      let list_append : list -> list -> list =
        let rec f1 (l1:list) : list -> list =
          fun (l2:list) ->
            match l1 with
              | Nil -> l2
              | Cons (n1, l3) -> Cons (n1, f1 l3 l2)
        in
          f1
      ;;
    \end{lstlisting}
  \end{center}
\end{minipage}

This final program is the standard implementation of $\mkwd{append}$.

\paragraph{Validation}

An immediate question one might ask about this process is: ``How do you validate that the programs that \myth{} produces are correct?''
Indeed, this proves to be a tricky issue.
Even though we proved soundness of the system (\autoref{lem:type-soundness-of-mlsyn} and \autoref{lem:example-soundness-of-mlsyn}), this only guarantees that the programs \myth{} produces are well-typed and satisfy the examples.
However, concrete examples only specify a finite subset of behavior which is insufficient to express the behavior of most functions we care to synthesis such as recursive functions.
We appeal to the Minimum Program Principle (\autoref{subsec:the-minimum-program-principle}) to maximize the likelihood that the synthesized program agrees with the behavior that the user intends with their examples.
However, at the end of the day, the user needs to validate for themselves that the resulting program meets their needs.

They might accomplish this by inspecting their program for obvious defects as we did in the above example.
Or they may test their program on a wider variety of examples than what they presented to \myth{}.
To validate our own examples, we went through this process.
Because we synthesized common recursive functional programs, we were able to verify the correctness of \myth{}'s output by inspection.
In a minority of cases (some of which we explore in \autoref{sec:extended-examples}), the synthesized program's correctness was non-obvious and required additional testing to verify.

This work flow, as is, has some clear downsides in the context of an end-user tool.
In particular, validation is problematic because in a real-world scenario, the user may not know what program they want in the end.
After all, they are using program synthesis to try to discover this program!
However, this work flow is adequate for discovering how our typed-directed synthesis algorithm works and its behavior on real world examples.

\section{Benchmark Suite}
\label{sec:benchmark-suite}

To assess the effectiveness of \myth{}, we created a synthesis benchmark suite consisting of 43 sample \myth{} programs developed using the methodology discussed in \autoref{sec:example-development}.
These programs provide specifications of simple functions over fundamental functional data types---booleans, natural numbers, lists, and trees---exercising all of the core functional programming features that \myth{} provides: inductive algebraic data types, higher-order functions, and recursion.
\begin{description}
  \item[Booleans:]
    Our simplest benchmarks involve synthesizing functions over booleans values.
    These programs include common boolean operators: $\mkwd{neg}$, $\mkwd{and}$, $\mkwd{or}$, $\mkwd{impl}$, and $\mkwd{xor}$.
    Because these boolean functions are all non-recursive, we can fully specify their behavior through straightforward case analysis with our examples, demonstrating the power of pure type-directed refinement.

  \item[Natural Numbers:]
    To graduate from the world from non-recursive functions to recursive functions, we introduce example programs over the most basic of inductive algebraic data types, the natural numbers:
    \[
      \mkwd{type}\;\mnat = \mO \bnfalt \mS\;\mkwd{of}\;\mnat.
    \]
    These functions include unary functions over $\mnat$s (\eg, $\mkwd{is\_even}$ and $\mkwd{prev}$) as well as binary operators over $\mnat$ (\eg, $\mkwd{max}$ and $\mkwd{sum}$).

  \item[Lists:]
    Expanding on $\mnat$s, the bulk of our benchmarks feature operations over $\mlist$s.
    \mlsyn{} and consequently \myth{} do not have polymorphism.
    Therefore, our $\mlist$ data type is \emph{monomorphic}, fixed to a particular carrier type, usually $\mnat$ or $\mBool$.
    Our example programs include simple operations over a single list or pair of lists (\eg, $\mkwd{length}$ and $\mkwd{append}$) as well as more complex operations (\eg, $\mkwd{compress}$ and $\mkwd{reverse}$).
    Note that because our type-directed synthesis style does not not admit synthesis of $\mkwd{let}$-bindings (\autoref{sec:let-binding}), we must provide the appropriate helper functions to synthesize these programs.
    In particular, we explore synthesizing $\mkwd{reverse}$ with a variety of approaches: ``cons-on-end'' ($\mkwd{snoc}$), $\mkwd{append}$, $\mkwd{fold}$, and a tail-recursive variant that requires a second argument.
    Finally, we use $\mlist$s to explore synthesis of higher-order functions, providing specifications for implementations of $\mkwd{map}$, $\mkwd{fold}$ and $\mkwd{filter}$ as well as usages of these higher-order functions as helpers.

  \item[Trees:]
    Finally, we also explore richer data types with functions over $\mtree$ data types which require richer sets of examples to capture their behavior.
    These operations include simple $\mtree$ processing functions such as counting the number of nodes and performing different sorts of traversals over a $\mtree$ as well as more complicated operations such as binary insertion (assuming that the $\mtree$ is a binary search tree).
\end{description}

The full text of all these sample \myth{} programs as well as the resulting synthesized programs can be found in \autoref{app:the-myth-test-suite}.

\subsection{Analysis}
\label{subsec:analysis}

Now, let us examine the results of running \myth{} over our benchmark suite.
In this section, we present performance numbers for an implementation of \myth{} in the OCaml programming language~\todo{cite---OCaml manual}.
This implementation was exercised on a desktop PC equipped with the openSUSE operating system (version 13.1), an Intel i7-3770 quad-core CPU clocked at 3.40 GHz, and 8 Gb of ram.
Note that because the version of OCaml we used features a runtime with a global lock, we were unable to take advantage of more than one core.
We expect that there are substantial performance benefits to moving to an implementation that can utilize multiple cores.
All benchmarks were run five times in succession, and we report the average runtime collected from those trials.

\input{figures/myth-raw-benchmarks}

\autoref{fig:myth-raw-benchmarks} presents the complete performance data of running \myth{} over our benchmark suite.
The benchmark suite contains 42 test programs.
On average, we provided \myth{} with 7 examples and it produced a program of size 13 in 0.092 seconds.
Since every program that we synthesize is a function, we count an example as a single collection of input/output examples that determines one execution of the function in question.
For example, we count the partial function we provided to \myth{} to synthesis the $\mkwd{append}$ function,
\begin{align*}
   & [] ⇒ ([] ⇒ [] \bnfalt [0] ⇒ [0]) \\
  \bnfalt & [0] ⇒ ([] ⇒ [0] \bnfalt [0] ⇒ [0, 0]) \\
  \bnfalt & [1,0] ⇒ ([] ⇒ [1, 0] \bnfalt [0] ⇒ [1, 0, 0]),
\end{align*}
as six examples, corresponding to the following executions of $\mkwd{append}$:
\begin{align*}
  \mkwd{append}\;[]\;[] &= [] \\
  \mkwd{append}\;[]\;[0] &= [0] \\
  \mkwd{append}\;[0]\;[] &= [0] \\
  \mkwd{append}\;[0]\;[0] &= [0, 0] \\
  \mkwd{append}\;[1, 0]\;[] &= [1, 0] \\
  \mkwd{append}\;[1, 0]\;[0] &= [0, 0, 0] \\
\end{align*}

\paragraph{Examples and Program Complexity}

\input{figures/myth-benchmarks-graphs-counts}

Let's take a deeper look at the data, first by examining the interplay between the number of examples required to synthesize a program and its final size.
From \autoref{sec:example-development}, we saw that examples in \myth{} have the practical effect of forcing \myth{} to discriminate between cases.
For example, two examples distinguishing between the base case and inductive case of a data type can result in a pattern match on an appropriate value.
Or, two different examples might force the synthesizer to choose an $E$-term involving variables and function application rather than a constant.
In either case, \myth{} discriminates between examples by adding complexity to the synthesized program in the form of additional language constructs.

Our hope is that our type-directed synthesis style requires a minimal amount of examples to create complex programs.
To assess this, in \autoref{fig:myth-benchmarks-graphs-counts} we graph the 42 benchmark programs against the number of examples used, the size of the resulting program, and the ratio of the synthesized program to the number of examples used.
These graphs better allow us to see trends in the data that we presented in table form in \autoref{fig:myth-raw-benchmarks}.
In particular, we see from the first graph that the average size of our synthesized programs scales with the complexity of the data types that we are operating over: $\mBool = 6.4$, $\mnat = 10$, $\mlist = 12.8$, and $\mtree = 19.75$ AST nodes.
However, aside from the non-recursive boolean programs whose examples completely determine their functionality (at an average of 3.6 examples per program), the second graph shows that we require roughly the same number of examples---6--7 such examples---to synthesize recursive functions over $\mnat$s, $\mlist$s, and $\mtree$s ($\mnat = 6.25$, $\mlist = 7$, and $\mtree = 7.29$).

Consequently, this means that it appears as our programs operate over more complex data types, we require comparatively less examples to produce more complex programs.
To see this, in the third graph we compare the ratio of synthesized program size to the number of examples used to synthesize that program.
While the ratios are fairly close between the $\mbool$, $\mnat$, and $\mlist$ programs ($\mbool = 1.78$, $\mnat = 1.6$, and $\mtree = 1.83$), the more complex $\mtree$ programs benefit more with each example that we provide ($\mtree = 2.71$).
The programs with the best ratios are \texttt{list\_stutter} (11 AST nodes, 3 examples, 3.67 size/example ratio) and \texttt{tree\_inorder} (15 AST nodes, 5 examples, 3.0 size/example ratio).
In particular, the \texttt{list\_stutter} we've studied throughout this work is ``optimal'' for $\mkwd{list}$ programs in the (informal) sense that we only need to present one example for each case of the $\mkwd{list}$ data type as well as an additional example for the $\mkwd{Cons}$ case to avoid synthesizing a constant.
\texttt{list\_stutter} also benefits greatly from $I$-refinement when synthesizing the $\mCons(x, \mCons(x, ◼))$ portion of the $\mCons$ branch because this program fragment is entirely dictated by the two examples that are distributed to it.
The program with the worst ratio is \texttt{list\_drop} (13 AST nodes, 11 examples, 1.18 size/example ratio) which provides particularly tricky because we must provide several additional examples to handle the partial behavior of the function when we try to drop more elements than the length of the input list.

\paragraph{Synthesis Speed}

\input{figures/myth-raw-benchmarks-graphs}

In addition to understanding how \myth{} uses examples, we would also like to understand how quickly it can synthesize programs.
\autoref{fig:myth-benchmarks-timing-graph} shows the time taken to synthesize each program of the benchmark suite, classified by the primary data type that the program operates over.
With two exceptions, \texttt{tree\_nodes\_at\_level} and \texttt{tree\_postorder}, all the benchmark programs synthesize in sub-second time, effectively instantaneous.
Looking at the average runtime of each class of programs, we see that the synthesis time scales with the complexity of the subject data type ($\mbool = 0.00061$, $\mnat = 0.00068$, $\mlist = 0.0029$, and $\mtree = 0.020$ seconds).
However, within each class, the run times vary wildly with a standard deviation of $0.19$ seconds across all the examples.
One might expect that the runtime ought to consistently scale with the size of the program, but this isn't the case because our refined synthesis procedure does not perform a breadth-first search.
By effectively adopting a hybrid depth-first/breadth-first search of the refinement tree, we are able to synthesis some programs that take advantage of $I$-refinements very quickly even though they are comparatively large.

In contrast, performance stalls when we are forced to $E$-guess terms whose size crosses the exponential barrier of term generation or explore lots of nested matches.
\texttt{tree\_nodes\_at\_level} and \texttt{tree\_postorder} both fall into the former category.
For example, \myth{} produces the following program for \texttt{tree\_nodes\_at\_level}

\begin{center}
  \begin{minipage}{\textwidth}
    \begin{lstlisting}
      let tree_nodes_at_level : tree -> nat -> nat =
       let rec f1 (t1:tree) : nat -> nat =
         fun (n1:nat) ->
           match t1 with
             | Leaf -> O
             | Node (t2, b1, t3) -> (match n1 with
                                       | O -> S (O)
                                       | S (n2) -> sum (f1 t3 n2) (f1 t2 n2))
       in
         f1
      ;;
    \end{lstlisting}
  \end{minipage}
\end{center}

The function application in the $\mS$ branch of the inner match has size 12 which takes significantly longer to generate.

\subsection{Context Size and Performance}
\label{subsec:context-size-and-performance}

\input{figures/myth-extended-context}

In the previous section, we synthesized the example programs in the minimal context necessary.
In practice, a synthesis tool can achieve this by allowing the user to choose which functions in scope ought to be usable during the synthesis process.
However, if there are many such functions available, or the user is unsure of what functions the program might use, we will need to synthesize in a rich context.
To simulate this, we also ran our benchmark suite in a context with a number of helper functions comparable to the prior work~\todo{cite---Escher, Leon, etc.}.
\autoref{fig:myth-extended-context} gives the signatures of the types and functions found in this context.
Whenever, a benchmark program is one of the functions found in the context, we remove that function from the context so we don't end up synthesizing the trivial invocation of that top-level function.

\input{figures/myth-raw-context}
\input{figures/myth-context-graphs}

\autoref{fig:myth-raw-context} compares the performance numbers between synthesizing the benchmark suite in a minimal context versus the extended context.
It also reports the percentage difference between synthesizing in the minimal versus extended contexts.
\autoref{fig:myth-context-graphs} provides plots of synthesis times in the extended context along with the percentage differences in execution time between minimal and extended contexts.
From the data, it is clear that the context has a very significant effect on the execution time of \myth{}.
Overall, there is a 101\% increase in performance moving from the minimal context to the extended context.
Thankfully, because the synthesis times were already sub-second, the median execution time remains imperceptible at $0.0234$ seconds.

However, several benchmarks experience a dramatic, noticeable change in run time.
The most egregious of these examples is \texttt{list\_compress} which jumps up to an average run time of 2.5 minutes!
Examining the output for \texttt{list\_compress} reveals the problem:

\begin{center}
  \begin{minipage}{0.5\textwidth}
    \begin{lstlisting}
let list_compress : list -> list =
  let rec f1 (l1:list) : list =
    match l1 with
    | Nil -> Nil
    | Cons (n1, l2) ->
      (match f1 l2 with
      | Nil -> l1
      | Cons (n2, l3) ->
        (match compare n2 n1 with
        | LT -> Cons (n1, Cons (n2, l3))
        | EQ -> Cons (n1, l3)
        | GT -> Cons (n1, Cons (n2, l3))))
in
  f1
;;
    \end{lstlisting}
  \end{minipage}
\end{center}

The triply nested match is relatively quick to synthesize in the minimal context because we only have $\mkwd{compare}$ with which to make complex match scrutinees.
However, with the addition of $\mkwd{append}$ and $\mkwd{plus}$ from the extended context, we now gain many more possible informative scrutinees that must be explored in the refinement tree.
The other cases where the execution time explodes, \eg, \texttt{tree\_binsert} and \texttt{list\_sorted\_insert}, work out similarly where deep matching is necessary, but the space of possible matches explodes with the extended context.

\section{Extended Examples}
\label{sec:extended-examples}

\input{figures/myth-extended-examples-results}

In addition to the benchmark suite, we also tested \myth{} on a variety of extended examples to better understand the limits of the system.
\autoref{fig:myth-extended-examples-results} gives an overview of the examples and their execution time with \myth{}.

\paragraph{Interpreters}

Typed, functional programming languages are excellent for writing compilers and interpreters because abstract syntax trees line up well algebraic data types.
As a result, our type-directed program synthesis style works well for synthesizing these sorts of programs.
To demonstrate this, we provide an example of synthesizing an interpreter for the following calculator language:

\begin{center}
  \begin{minipage}{0.25\textwidth}
    \begin{lstlisting}
type exp =
| Const of nat
| Sum of exp * exp
| Prod of exp * exp
| Pred of exp
| Max of exp * exp
    \end{lstlisting}
  \end{minipage}
\end{center}

The interpreter for this language has size 47 and requires 22 examples to synthesize in approximately 12 seconds.

\begin{center}
  \begin{minipage}{0.75\textwidth}
    \begin{lstlisting}
let arith : exp -> nat =
  let rec f1 (e1:exp) : nat =
    match e1 with
      | Const (n1) -> n1
      | Sum (e2, e3) -> sum (f1 e2) (f1 e3)
      | Prod (e2, e3) -> mult (f1 e2) (f1 e3)
      | Pred (e2) -> (match f1 e2 with
                        | O -> O
                        | S (n1) -> n1)
      | Max (e2, e3) -> (match compare (f1 e2) (f1 e3) with
                           | LT -> f1 e3
                           | EQ -> f1 e3
                           | GT -> f1 e2)
  in
    f1
;;
    \end{lstlisting}
  \end{minipage}
\end{center}

It utilizes several helper functions, $\mkwd{sum}$, $\mkwd{mult}$, and $\mkwd{compare}$, and the function itself contains multiple nested pattern matches with complex scrutinees which, from \autoref{subsec:analysis}, explains why it takes significantly longer to synthesize than the benchmark suite's programs.

\paragraph{Type Dynamic}

Recall from \autoref{subsec:static-and-dynamic-semantics-of-mlsyn} that part of our checks to ensure that \mlsyn{} is well-founded is enforcing a positivity restriction on data types.
This restrictions says that a recursive occurrence of a data type cannot appear to the left of an arrow in the type of any of its constructors.
For example, consider the data type encoding type dynamic~\todo{cite---dynamic work}:
\[
  \begin{array}{l}
    \mkwd{type}\;\mdyn = \\
    \bnfalt \mError \\
    \bnfalt \mBase\;\mof\;\mnat \\
    \bnfalt \mDyn\;\mof\;\mdyn → \mdyn
  \end{array}
\]
$\mdyn$ provides dynamic typing functionality over $\mnat$s ($\mBase$) and functions ($\mDyn$).
This data type breaks the positivity restriction because of the signature of $\mDyn$, so we would not allow it in \mlsyn{}.

However, as we showed in \autoref{subsec:static-and-dynamic-semantics-of-mlsyn}, the simplest infinite loop you can produce using this loophole requires the use of an external function (call it $f : \mdyn → \mdyn$) and its subsequent double application $f\,(D\,f)$.
Because \mlsyn{} does not have $\mlet$-bindings, we must synthesize a much larger function to produce this infinite loop, if it is even possible to produce in \mlsyn{} sans positivity restriction at all!
It is very likely that we will not encounter this program during normal execution of \myth{} because if such a program with an infinite loop exists, it is likely too complicated for \myth{} synthesize in practice.

In light of this revelation, we left out the positivity restriction from \myth{} and attempted to synthesize a pair of recursive functions over $\mdyn$, \texttt{dyn\_app\_twice} which applies a $\mDyn$ function twice and \texttt{dyn\_sum} which performs the addition operation through $\mdyn$s.
In either case, we produce $\mError$ if the types do not line up appropriately.
In both cases we are able to synthesize the correct function, lending weight to the argument that we would not encounter infinite loops via the positivity restriction in practice.
In particular, \texttt{dyn\_sum} is a particular hairy function, requiring two triply nested pattern matches to correctly raise $\mError$ at all the correct points in the program.

\begin{center}
  \begin{minipage}{0.75\textwidth}
    \begin{lstlisting}
let dyn_sum : dyn -> dyn -> dyn =
  fun (d1:dyn) ->
    fun (d2:dyn) ->
      match d1 with
        | Error -> Error
        | Base (n1) -> (match n1 with
                          | O -> (match d2 with
                                    | Error -> Error
                                    | Base (n2) -> d2
                                    | Dyn (f3) -> Error)
                          | S (n2) -> (match n2 with
                                         | O -> succ d2
                                         | S (n3) -> succ (succ d2)))
        | Dyn (f3) -> Error
;;
    \end{lstlisting}
  \end{minipage}
\end{center}

Note that this pattern of programming is similar to error handling with an option/maybe type.
In lieu of additional constructs to avoid excessive match expressions, we would need to use deeply nested pattern matches to crack open option/maybe values.

\paragraph{Free Variables Collector}

\myth{} combines depth-first and breadth-first search of the refinement tree in a particular way: first it efficiently computes the possible set of $I$-refinement all at once and then it $E$-guesses at the leaves.
This makes \myth{} very efficient when synthesizing programs that feature shallow pattern matches over large data types.
To illustrate this point, we synthesize programs that collect the free variables of a simply typed lambda calculus encoded in a locally nameless style~\todo{cite---locally nameless work}.
The largest such program, \texttt{fvs\_large}, operates over the following data type:

\begin{center}
  \begin{minipage}{0.75\textwidth}
    \begin{lstlisting}
type exp =
  | Unit
  | BVar of nat
  | FVar of nat
  | Lam of nat * exp
  | App of exp * exp
  | Pair of exp * exp
  | Fst of exp
  | Snd of exp
  | Inl of exp
  | Inr of exp
  | Const of nat
  | Add of exp * exp
  | Sub of exp * exp
  | Mult of exp * exp
  | Div of exp * exp
    \end{lstlisting}
  \end{minipage}
\end{center}

Passing the necessary 31 examples to \myth{} produces the following function in 3.024 seconds:

\begin{center}
  \begin{minipage}{0.75\textwidth}
    \begin{lstlisting}
let fvs_large : exp -> list =
  let rec f1 (e1:exp) : list =
    match e1 with
      | Unit -> []
      | BVar (n1) -> []
      | FVar (n1) -> [n1]
      | Lam (n1, e2) -> f1 e2
      | App (e2, e3) -> append (f1 e2) (f1 e3)
      | Pair (e2, e3) -> append (f1 e2) (f1 e3)
      | Fst (e2) -> f1 e2
      | Snd (e2) -> f1 e2
      | Inl (e2) -> f1 e2
      | Inr (e2) -> f1 e2
      | Const (n1) -> []
      | Add (e2, e3) -> append (f1 e2) (f1 e3)
      | Sub (e2, e3) -> append (f1 e2) (f1 e3)
      | Mult (e2, e3) -> append (f1 e2) (f1 e3)
      | Div (e2, e3) -> append (f1 e2) (f1 e3)
  in
    f1
;;
    \end{lstlisting}
  \end{minipage}
\end{center}

While this program is larger than any that we have synthesized so far, it is not the slowest by far.
This is because even though the match is tall, we discover it via $I$-refinement and synthesize the relatively thin branches (maximum size 9) through $E$-guessing very quickly.
Note that we are unable to synthesize the additional branch with a $\mkwd{Match}$ constructor with our na\"{i}ve parameter search strategy because we would need the $E$-guess the function application $\mkwd{append}\,(f1 e1)\,(\mkwd{append}\,(f1 e2)\,(f1 e3))$ of size 15, and \myth{} times out before it can do so.
With a better tuned search strategy, synthesizing these larger branches is certainly possible for \myth{}

\paragraph{Inside-out Recursion}

A hallmark of a good program synthesis tool is that can produce surprising results.
To illustrate this, we close by examining a more complicated example from our benchmarks, \texttt{list\_pairwise\_swap}, which swaps consecutive pairs of elements in a list.
When the list has odd length, we choose to return the empty list.
When we provide \myth{} with the following set of examples,
\[
  \begin{array}{l}
    & [] ⇒ []
    \bnfalt & [0] ⇒ []
    \bnfalt & [1] ⇒ []
    \bnfalt & [1,0] ⇒ [0,1]
    \bnfalt & [0,1] ⇒ [1,0]
    \bnfalt & [0,1,0,1] ⇒ [1,0,1,0],
  \end{array}
\]
it produces the following function:

\begin{center}
  \begin{minipage}{0.95\textwidth}
    \begin{lstlisting}
let list_pairwise_swap : list -> list =
  let rec f1 (l1:list) : list =
    match l1 with
      | Nil -> []
      | Cons (n1, l2) -> (match l2 with
                            | Nil -> []
                            | Cons (n2, l3) -> Cons (n2, Cons (n1, f1 l3)))
  in
    f1
;;
    \end{lstlisting}
  \end{minipage}
\end{center}

This implementation seems to be correct as it performs the expected double pattern match, swaps the two head elements, and recursively swaps the rest.
However, in the case where we call the function with the list $[1, 0, 1, 0, 1]$, it produces the list $[0, 1, 0, 1]$, truncating the last element off the list rather than returning $[]$.
To remedy this, we provide one additional example to \myth{}, $[1, 0, 1] ⇒ []$, and it now produces the correct result:

\begin{center}
  \begin{minipage}{0.95\textwidth}
    \begin{lstlisting}
let list_pairwise_swap : list -> list =
  let rec f1 (l1:list) : list =
    match l1 with
      | Nil -> []
      | Cons (n1, l2) -> (match f1 l2 with
                            | Nil -> (match l2 with
                                        | Nil -> []
                                        | Cons (n2, l3) -> Cons (n2,
                                                             Cons (n1, f1 l3)))
                            | Cons (n2, l3) -> [])
  in
    f1
;;
    \end{lstlisting}
  \end{minipage}
\end{center}

Rather than immediately pattern matching on the list twice, \texttt{list\_pairwise\_swap} instead pattern matches on \emph{a recursive call to the function}.
This seemingly odd behavior has a surprising effect: $f1\,l2$ becomes a test to see if the list has even ($\mNil$) length or odd length $(\mCons)$!
In the case when the list has odd length, we return $[]$, otherwise we proceed as normal.

In many situations, \myth{} elects to perform this behavior, a phenomenon we call \emph{inside-out recursion}, because it turns out to save an AST node (see \texttt{list\_compress}, \texttt{list\_even\_parity}, and \texttt{tree\_postorder}).
However, it turns out this behavior is necessary to implement \texttt{list\_pairwise\_swap} correctly because in the absence of $\mlet$-bindings or a helper function to judge whether the list has odd length, there is no other way to perform the necessary test.
In this sense, even though \myth{} is performing relatively na\"{i}ve search, it is still able to produce results that are surprising, yet correct, at first glance!
