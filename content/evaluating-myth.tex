In \autoref{ch:implementation}, we developed an efficient synthesis procedure from our core synthesis calculus \mlsyn{}.
In this chapter, we explore our implementation of this synthesis procedure, a prototype program synthesizer called \myth{}.\footnote{%
  We provide a summary of the final synthesis procedure as a reference in \autoref{app:the-implementation-of-myth}.
}

Our goal with \myth{} is to further explore the type-theoretic foundations for program synthesis that we have developed so far.
We started our exploration by carefully analyzing the metatheory of type-directed program synthesis, in particular the soundness and completeness of \lsyn{} and \mlsyn{}.
However, this is insufficient for getting a complete sense of how program synthesis systems built up top of these foundations will perform in practice.
By exploring the behavior of an actual implementation, we can better understand the capabilities and limitations of our approach and identify areas for future improvement.

Note that an explicit non-goal of this exploration is to justify \myth{}-the-artifact as a practical tool for program synthesis.
While we explore some aspects of the viability of \myth{} as an end-user tool, \eg, performance, we intentionally do not explore the usability of the tool.
We do this primarily as a matter of pragmatics.
There are plenty of empirical questions to investigate about \myth{}---How many examples do we need to synthesize a particular program?  How long does it take to synthesize a particular program?---without delving into the usability side of the project.
However, we also want to stress that, throughout this work, we have been less concerned with building a practical tool and more interested in answering foundational questions about the integration of types into program synthesis.
We do not want to overshadow these important results with claims about usability that we do not have the time to develop thoroughly.
We leave such investigation to future work.

\section{Search Parameter Tuning}
\label{sec:search-parameter-tuning}

Originally we started with a simple synthesis procedure that searched the space of programs according to program size in a breadth-first manner.
When we introduced refinement trees, we also introduced a number of search parameters into the procedure:
\begin{itemize}
  \item $s$, the maximum scrutinee size of any match expression,
  \item $m$, the maximum match depth, \ie, the maximum number of matches that can appear in any branch of a refinement tree, and
  \item $k$, the maximum size of $E$-terms that we generate during the $E$-guess phase of the procedure.
\end{itemize}
Rather than performing a breadth-first search by program size alone, we now perform a breadth-first search according to these three parameters.
Thus, choosing appropriate \emph{initial values} along with a \emph{strategy} for traversing through successive iterations of the algorithm is imperative towards obtaining good synthesis results.

\paragraph{Scrutinee Size}
Let's consider the effects of each of these parameters on the programs that the synthesis algorithm produces.
First, the scrutinee size affects the complexity of the pattern matches that we can synthesize.
At $s = 1$, we can only synthesize variables as scrutinees.
This has been sufficient for the examples we have used so far, but in general we need the ability to synthesize richer scrutinees.
As an example, suppose in OCaml that we have the standard $\mkwd{lookup}$ function of type $'a → ('a, 'b)\;\mlist → 'b\;\mkwd{option}$ in our context.
Then we need richer scrutinees to synthesize programs that use association lists such as:

\begin{center}
  \begin{minipage}{0.75\textwidth}
    \begin{lstlisting}
      match l with
      | Some r -> (* Process the result *)
      | None   -> (* Return some default value *)
    \end{lstlisting}
  \end{minipage}
\end{center}

However, we want to avoid opening the door to complex scrutinees too quickly.
This is because the number of possible scrutinees (and consequently, the number of possible matches) grows exponentially with the scrutinee size and very often, we only need to pattern match over a single variable.
Our informativeness restriction rules out some of these scrutinees, but in practice, many scrutinees make it through the restriction, requiring us to add these branches to our refinement tree.
These additional branches become more points where we have to $E$-guess, making synthesis more costly.
Furthermore, on top of these scrutinees, as we increase the scrutinee size $s$, we also increase the liklihood of generating scrutinees that are equivalent to previously generated scrutinees.

\paragraph{Match Depth}
Related to the scrutinee size, the match depth $m$ controls the number of match expressions that can appear in any branch of the refinement tree.
Whereas the scrutinee size controls the \emph{width} of matches, \ie, the amount of possible match expressions at any point in a refinement tree, the match depth controls the \emph{depth} of matches, \ie, how deeply we can nest pattern matches in a candidate program.
In many cases, a single match expression is sufficient to satisfy a given synthesis problem.
However, sometimes we may also need to resort to nested matching to access successive elements of a structure or to make decisions based off of case analysis of multiple pieces of data.
The danger is that unnecessary nested matches greatly impact performance because in addition to the branching factor, each branch introduces additional binders which accelerates the exponential blow-up associated with raw-term enumeration.
Thankfully, we rule out equivalent, nested matches thanks to our informativeness restriction (see \autoref{subsec:reigning-in-matches}), although the cost of stacking up binders due to nested matches is very significant.

An additional problem with setting the match depth too high is overspecialization.
For example, consider the overspecializing function for $\mkwd{stutter}$ from \autoref{subsec:the-minimum-program-principle}.
\[
  \begin{array}{l}
    \mfix\;f\;(l{:}\mlist) : \mlist = \\
    \⇥{1} \mmatch\;l\;\mwith \\
    \⇥{1}   \bnfalt \mNil → [] \\
    \⇥{1}   \bnfalt \mCons(x, l') → \mmatch\;l'\;\mwith \\
    \⇥{2}   \bnfalt \mNil → [0, 0] \\
    \⇥{2}   \bnfalt \mCons(y, l'') → \mmatch\;l''\;\mwith \\
    \⇥{3}   \bnfalt \mNil → [1, 1, 0, 0] \\
    \⇥{3}   \bnfalt \mCons(z, l''') → [].
  \end{array}
\]
By synthesizing programs in order of increasing size, we guaranteed that our original synthesis procedure would not synthesize this program before synthesizing the desired recursive function.
However, imagine in our new synthesis procedure that we started with the match depth $m = 3$.
Notice that because overspecialization contains no $E$-guessed terms, our initial refinement tree would contain this program immediately and we would be done!
Because of this, we have to start with a small match depth and be careful about increasing it too quickly.

\paragraph{$E$-term Size}
Finally, $k$ controls the maximum size of $E$-terms that the algorithm is allowed to guess at any node in the refinement tree.
The primary benefit of our refinement tree structure, is that we have localized the expensive procedure of $E$-term enumeration to the leaves of the refinement tree.
This allows us to generate much smaller $E$-terms on average.
For example, in our original algorithm, when we synthesized terms up to size 11, we would need to explore the space of $E$-terms of size 11 if our context allowed us to generate $E$-terms of that size.
In our updated algorithm, rather than having to synthesize $E$-terms up to size 11 to find $\mkwd{stutter}$, we only need to synthesize $E$-terms up to size 3 to find the recursive function call $f\,l'$.
This is a tremendous win in terms of efficiency because of the exponential cost of enumerating large $E$-terms.

Unlike match depth, we do not run into issues of overspecialization if $k$ begins too large.
In fact, by admitting larger $E$-terms, we use more variables and function applications which, by our Minimum Programming Principle (\autoref{subsec:the-minimum-program-principle}) makes it more likely our synthesized program generalizes to the behavior that the user intends.
Furthermore, the cost of synthesizing $E$-terms of modest size is very small.
It is not until we reach the critical point in the exponential cliff that generation time jumps from sub-seconds to minutes.
So we are safe in starting with an $E$-term size of reasonable size.

\subsection{Search Strategy}

With these considerations in mind, we can now discuss the particular search strategy we implemented in \myth{}.
With multiple search parameters, we could devise an \emph{adaptive} strategy that responds to the performance of the synthesizer for the particular synthesize problem at hand.
For example, we may vary the initial $E$-term size $k$ or how much we increment the $k$ depending on the size of the context we synthesize under.
Because our context grows as we move down in the refinement tree as we add binders with functions or matches, we would need to vary the search parameters at different points in the tree.

For these experiments, we opted for a simpler \emph{static} strategy suitable for the domain of programs we intended to synthesize---simple functional programs over recursive data types such as lists and trees.  We begin with the following initial search parameter values:
\begin{itemize}
  \item The maximum scrutinee size, $s = 1$.
  \item The maximum match depth, $m = 0$.
  \item The maximum $E$-guess size, $k = 13$.
\end{itemize}
We then move through the following phases of $I$-refinement.
In each phase of $I$-refinement, we increase one of our search parameters and then extend the refinement tree accordingly.
In particular, when we increase the match depth or scrutinee size, we traverse the refinement tree, create new match expressions as necessary, and carry out further $I$-refinements (in particular, application of \rulename{irefine-arr} and \rulename{irefine-base}) where ever possible.
Finally, after each $I$-refinement phase, we allocate 0.25 seconds to generates $E$-terms up to the size limit $k$ at each eligible refinement tree node and then propagate the results to see if we can synthesize a complete, satisfying program.

The phases of $I$-refinement we go through are:
\begin{enumerate}
  \item Create an initial refinement tree with the search parameters $s$ and $m$.
  \item Increase $m$ by one ($m = 1$) and extend the refinement tree accordingly.
  \item Increase $m$ by one ($m = 2$) and extend the refinement tree accordingly.
  \item Increase $s$ by five ($s = 6$) and extend the refinement tree accordingly.
  \item Increase $m$ by one ($m = 3$).
\end{enumerate}
By the end of the process, we synthesize programs with $E$-terms of size 13, triply-nested pattern matches, with scrutinees up to size six.
This is simple strategy is sufficient to handle all of the examples that we discuss in \autoref{sec:benchmark-suite}.

\section{Example Development}
\label{sec:example-development}

\input{figures/myth-stutter}

The input to the \myth{} synthesizer is a series of top-level declarations in the subset of OCaml identified by \mlsyn{} along with a synthesis problem---a name, goal type, and example values.
For example, \autoref{fig:myth-stutter} gives the source code for the $\mkwd{stutter}$ in \myth{} that we have studied extensively in the previous chapters.
\myth{} has no built-in data types, so we must provide them all ourselves using algebraic data types.
Here, we declare a type of monomorphic $\mlist$s whose carrier type is $\mnat$.
The declaration of $\mkwd{stutter}$ contains the familiar three input/output examples\footnote{%
  For sake of convenience, \myth{} provides syntactic sugar for representing lists and natural numbers.
}
suggesting the intended behavior of $\mkwd{sutter}$.

We arrived at this set of examples by using the following process:
\begin{enumerate}
  \item We observed that the likely data type we need to perform induction over was $\mlist$, so we provided examples of how the function should work in both cases of $\mlist$ as defined its constructors: $[] ⇒ [] \bnfalt [0] ⇒ [0; 0]$.
  \item We ran \myth{} with these examples, obtaining the program:

    \begin{minipage}{0.5\textwidth}
      \begin{center}
        \begin{lstlisting}
          let list_stutter : list -> list =
            fun (l1:list) -> match l1 with
                               | Nil -> []
                               | Cons (n1, l2) -> [0; 0]
          ;;
        \end{lstlisting}
      \end{center}
    \end{minipage}

    Upon inspection, this program is not sufficient because while it has the correct behavior in the $\mNil$ case, it does not have the correct behavior in the $\mCons$ case, returning a constant list rather than performing a more general calculation.

  \item To fix this problem, we need to add additional examples to rule out this expression in the $\mCons$ branch.
    Keeping in mind our trace completeness restriction to power recursive function calls (\autoref{subsec:trace-completeness}), we add
    one additional example that is trace complete with respect to the original examples, $[1; 0] ⇒ [1; 1; 0; 0]$, and see if that creates a satisfactory program.
    Updating our program with this additional example and running it through \myth{} yields:

    \begin{minipage}{0.6\textwidth}
      \begin{center}
        \begin{lstlisting}
          let list_stutter : list -> list =
            let rec f1 (l1:list) : list =
              match l1 with
                | Nil -> []
                | Cons (n1, l2) -> Cons (n1, Cons (n1, f1 l2))
            in
              f1
          ;;
        \end{lstlisting}
      \end{center}
    \end{minipage}

    Which is the standard implementation of $\mkwd{stutter}$, so we are done!
\end{enumerate}

We use this process of iterative example refinement based on the inductive structure of the data type to develop all of the programs we supply to \myth{}.

\paragraph{Multi-argument Functions}

When synthesizing single argument functions, we can perform case analysis using our examples as described above in a straight forward manner.
However, the process becomes more complex with multi-argument functions.
In general, we will need additional examples demonstrating how our function behaves with each argument.

As a simple example, consider synthesizing the $\mkwd{append}$ function of type $\mlist → \mlist → \mlist$.
Since the only argument type is $\mlist$, it makes sense that we ought to perform case analysis on one of the $\mlist$ arguments.
Let's choose the first argument and try the following examples:
\begin{align*}
 & [] ⇒ [] ⇒ [] \\
\bnfalt & [0] ⇒ [] ⇒ [0] \\
\bnfalt & [1,0] ⇒ [] ⇒ [1,0]
\end{align*}
Feeding these examples to \myth{} produces the following function

\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{lstlisting}
      let list_append : list -> list -> list =
        fun (l1:list) -> fun (l2:list) -> l1
      ;;
    \end{lstlisting}
  \end{center}
\end{minipage}

This is not the $\mkwd{append}$ function we want; it is the function that always chooses its first argument!
Of course, looking at the examples we provided, this is certainly the simplest program that satisfies the examples.

The problem is that we have not specified the behavior of $\mkwd{append}$ on non-trivial second arguments.
Let's try doing so incrementally, first introducing a single extra example:
\begin{align*}
 & [] ⇒ ([] ⇒ [] \bnfalt [0] ⇒ [0]) \\
\bnfalt & [0] ⇒ [] ⇒ [0] \\
\bnfalt & [1,0] ⇒ [] ⇒ [1,0]
\end{align*}
Running \myth{} on these examples yields the refined function:

\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{lstlisting}
      let list_append : list -> list -> list =
        fun (l1:list) ->
          fun (l2:list) -> match l2 with
                             | Nil -> l1
                             | Cons (n1, l3) -> Cons (0, l1)
      ;;
    \end{lstlisting}
  \end{center}
\end{minipage}

This function is closer to $\mkwd{append}$ but still not there.
In particular, it only append a $0$ onto $l1$ in the $\mCons$ case, completely ignoring $l2$ in the process.
Adding another example to handle when we call $\mkwd{append}\,[0]\,[0]$ results in the same program.
We must add one additional example,
\begin{align*}
 & [] ⇒ ([] ⇒ [] \bnfalt [0] ⇒ [0]) \\
\bnfalt & [0] ⇒ ([] ⇒ [0] \bnfalt [0] ⇒ [0, 0]) \\
\bnfalt & [1,0] ⇒ ([] ⇒ [1, 0] \bnfalt [0] ⇒ [1, 0, 0]).
\end{align*}
On these examples \myth{} produces the program

\begin{minipage}{0.5\textwidth}
  \begin{center}
    \begin{lstlisting}
      let list_append : list -> list -> list =
        let rec f1 (l1:list) : list -> list =
          fun (l2:list) ->
            match l1 with
              | Nil -> l2
              | Cons (n1, l3) -> Cons (n1, f1 l3 l2)
        in
          f1
      ;;
    \end{lstlisting}
  \end{center}
\end{minipage}

This final program is the standard implementation of $\mkwd{append}$.

\paragraph{Validation}

An immediate question one might ask about this process is: ``How do you validate that the programs that \myth{} produces are correct?''
Indeed, this proves to be a tricky issue.
Even though we proved soundness of the system (\autoref{lem:type-soundness-of-mlsyn} and \autoref{lem:example-soundness-of-mlsyn}), this only guarantees that the programs \myth{} produces are well-typed and satisfy the examples.
However, concrete examples only specify a finite subset of behavior which is insufficient to express the behavior of most functions we care to synthesis such as recursive functions.
We appeal to the Minimum Program Principle (\autoref{subsec:the-minimum-program-principle}) to maximize the likelihood that the synthesized program agrees with the behavior that the user intends with their examples.
However, at the end of the day, the user needs to validate for themselves that the resulting program meets their needs.

They might accomplish this by inspecting their program for obvious defects as we did in the above example.
Or they may test their program on a wider variety of examples than what they presented to \myth{}.
To validate our own examples, we went through this process.
Because we synthesized common recursive functional programs, we were able to verify the correctness of \myth{}'s output by inspection.
In a minority of cases (some of which we explore in \autoref{sec:extended-examples}), the synthesized program's correctness was non-obvious and required additional testing to verify.

This work flow, as is, has some clear downsides in the context of an end-user tool.
In particular, validation is problematic because in a real-world scenario, the user may not know what program they want in the end.
After all, they are using program synthesis to try to discover this program!
However, this work flow is adequate for discovering how our typed-directed synthesis algorithm works and its behavior on real world examples.

\section{Benchmark Suite}
\label{sec:benchmark-suite}

To assess the effectiveness of \myth{}, we created a synthesis benchmark suite consisting of 43 sample \myth{} programs developed using the methodology discussed in \autoref{sec:example-development}.
These programs provide specifications of simple functions over fundamental functional data types---booleans, natural numbers, lists, and trees---exercising all of the core functional programming features that \myth{} provides: inductive algebraic data types, higher-order functions, and recursion.
\begin{description}
  \item[Booleans:]
    Our simplest benchmarks involve synthesizing functions over booleans values.
    These programs include common boolean operators: $\mkwd{neg}$, $\mkwd{and}$, $\mkwd{or}$, $\mkwd{impl}$, and $\mkwd{xor}$.
    Because these boolean functions are all non-recursive, we can fully specify their behavior through straightforward case analysis with our examples, demonstrating the power of pure type-directed refinement.

  \item[Natural Numbers:]
    To graduate from the world from non-recursive functions to recursive functions, we introduce example programs over the most basic of inductive algebraic data types, the natural numbers:
    \[
      \mkwd{type}\;\mnat = \mO \bnfalt \mS\;\mkwd{of}\;\mnat.
    \]
    These functions include unary functions over $\mnat$s (\eg, $\mkwd{is\_even}$ and $\mkwd{prev}$) as well as binary operators over $\mnat$ (\eg, $\mkwd{max}$ and $\mkwd{sum}$).

  \item[Lists:]
    Expanding on $\mnat$s, the bulk of our benchmarks feature operations over $\mlist$s.
    \mlsyn{} and consequently \myth{} do not have polymorphism.
    Therefore, our $\mlist$ data type is \emph{monomorphic}, fixed to a particular carrier type, usually $\mnat$ or $\mBool$.
    Our example programs include simple operations over a single list or pair of lists (\eg, $\mkwd{length}$ and $\mkwd{append}$) as well as more complex operations (\eg, $\mkwd{compress}$ and $\mkwd{reverse}$).
    Note that because our type-directed synthesis style does not not admit synthesis of $\mkwd{let}$-bindings (\autoref{sec:let-binding}), we must provide the appropriate helper functions to synthesize these programs.
    In particular, we explore synthesizing $\mkwd{reverse}$ with a variety of approaches: ``cons-on-end'' ($\mkwd{snoc}$), $\mkwd{append}$, $\mkwd{fold}$, and a tail-recursive variant that requires a second argument.
    Finally, we use $\mlist$s to explore synthesis of higher-order functions, providing specifications for implementations of $\mkwd{map}$, $\mkwd{fold}$ and $\mkwd{filter}$ as well as usages of these higher-order functions as helpers.

  \item[Trees:]
    Finally, we also explore richer data types with functions over $\mtree$ data types which require richer sets of examples to capture their behavior.
    These operations include simple $\mtree$ processing functions such as counting the number of nodes and performing different sorts of traversals over a $\mtree$ as well as more complicated operations such as binary insertion (assuming that the $\mtree$ is a binary search tree).
\end{description}

The full text of all these sample \myth{} programs as well as the resulting synthesized programs can be found in \autoref{app:the-myth-test-suite}.

\subsection{Analysis}
\label{subsec:analysis}

Now, let us examine the results of running \myth{} over our benchmark suite.
In this section, we present performance numbers for an implementation of \myth{} in the OCaml programming language~\todo{cite---OCaml manual}.
This implementation was exercised on a desktop PC equipped with the openSUSE operating system (version 13.1), an Intel i7-3770 quad-core CPU clocked at 3.40 GHz, and 8 Gb of ram.
Note that because the version of OCaml we used features a runtime with a global lock, we were unable to take advantage of more than one core.
We expect that there are substantial performance benefits to moving to an implementation that can utilize multiple cores.
All benchmarks were run five times in succession, and we report the average runtime collected from those trials.

\input{figures/myth-raw-benchmarks.tex}

\input{figures/myth-raw-benchmarks-graphs}

\autoref{fig:myth-raw-benchmarks} presents the complete performance data of running \myth{} over our benchmark suite.
The benchmark suite contains 42 test programs.
On average, we provided \myth{} with seven examples and it produced a program of size 13 in 0.092 seconds.
The average execution time is inflated by a pair of benchmarks (\texttt{tree\_nodes\_at\_level} and \texttt{tree\_postorder}) that took longer than a second to execute.
The median execution time, 0.0075 seconds, is more indicative of \myth{}'s performance over these benchmarks.

We graph relevant data in \autoref{fig:myth-benchmark-graphs} to better visualize the trends.

\subsection{Context Size and Performance}
\label{subsec:context-size-and-performance}

\section{Extended Examples}
\label{sec:extended-examples}
