Program synthesis is an undecidable problem, so in building practical synthesis tools, we must resort to approximations and heuristics (either in the problem domain or the synthesis algorithms) to not only make the problem solvable, but also tractable.
However, this reality is frequently at odds with building a system that is understandable as the approximations and heuristics frequently bleed together so that their individual contributions are not clear.
Ideally, we would like to build synthesis systems in such a way that we know precisely the impact of each feature and design choice.
For example, does ruling out a certain class of programs impact the completeness of the synthesis algorithm?
If so, are these programs relevant or are they safe to ignore for practical purposes?

To this end, we initially build a theoretical foundation for program synthesis types.
In typical programming language theory style, we strip away everything but the essential components of typed, functional programming languages by starting with the simply-typed lambda calculus, \stlc{}.
From \stlc{}, we build a generator for well-typed $λ$-terms and then integrate a notion of specification into the system to create a program synthesis calculus, \lsyn{}.

\lsyn{} itself is far removed from a usable implementation of a program synthesizer.
However, it allows us to deeply study how to synthesize programs with types, and it enjoys strong meta-theoretic properties, \ie, \lsyn{} is sound and complete with respect to synthesis.
In successive chapters, we evolve \lsyn{} into a practical program synthesizer for ML-like programs, sacrificing some of these properties to handle advanced types and gain efficiency.

\input{figures/stlc-defn.tex}

\section{Generating \texorpdfstring{$λ$}{λ}-terms}

\autoref{fig:stlc-defn} gives the syntax and semantics for \stlc{} which contains the essence of a typed, functional programming language---variables, functions, and application---and their usual type checking rules.
In addition to these essentials, we also augment \stlc{} with a base type $T$ and a finite set of constants $c_1, …, c_k$ of type $T$.
While these constants are not necessary, they allow us to synthesize more interesting programs than just those involve functions and a $\mkwd{Unit}$ type.
We also define a small-step, call-by-value operational semantics for the language using evaluation contexts $ℰ$~\todo{cite}.

To generate \stlc{} terms, we could simply enumerate terms according to the grammar given in \autoref{fig:stlc-defn}.
Clearly, there are an infinite number of terms, but we can make the enumeration deterministic by enumerating in order of term size, \ie, abstract syntax tree (AST) node count.
For example, the term $λx{:}T.\,y\;x$ has size 5 because the type $T$ has size one, the application $y\;x$ has size 3, and the lambda itself contributes one additional AST node.

\input{figures/lambda-terms-counts.tex}

However, simple enumeration is not very practical, especially when we have the type system at our disposal.
To see why, consider the number of closed \stlc-terms at a given size, \ie, terms generated from the empty typing context.
We define the size of a term to be the number of nodes in its AST according to the grammar from \autoref{fig:stlc-defn}.
\autoref{fig:lambda-terms-counts} gives the number of such untyped terms, well-typed terms, and well-typed terms at type $T$ when there are two constants $c_1$ and $c_2$ of type $T$.
For example:
\begin{itemize}
  \item $c_1\;(λx{:}T.\,x)$ is an syntactically valid, yet ill-typed term of size five.
  \item $c_2$ is a well-typed term of type $T$ and size one.
  \item $(λx{:}T → T.\,x)\;(λx{:}T.\,x)$ is a well-typed term of type $T → T$ of size nine.
\end{itemize}

Noting that the $y$-axis scale of \autoref{fig:lambda-terms-counts} is at logscale, we can see that the number of syntactically valid, but not necessarily well-typed terms is staggering.
Even in this extremely limited language, there are 11,084,176 such terms at size 15!
In contrast, if we limit our consideration to only the well-typed terms, we save an order of magnitude of work; there are only 1,439,481 well-typed terms of size 15 in \stlc.
\todo{Cite the counting lambda terms work.}
Finally, if we consider only well-typed programs, we may as well further refine our search to programs of a particular type that we are interested in.
The savings here is even larger; if we are trying to find terms of type $T → T$, we only have to search 1,489 terms.

We could refine our term generation strategy by enumerating syntactically well-typed terms and then filtering out terms that fail to type check.
This has the benefit of ensuring that any terms that we keep around will be well-typed.
However, we still pay the time and space overhead of generating such terms and then type checking them, even if we end up throwing them away in the end.
This is undesirable in the presence of the steep combinatorial explosion of terms as size increases.
Rather than type check after the fact, we should \emph{integrate} type checking into term enumeration so that we only ever consider well typed terms.

To arrive at such an algorithm, we start with the type checking judgment presented in \autoref{fig:stlc-defn}:
\[
Γ ⊢ e : τ.
\]
When implementing a type checker based on this judgment, we note that $Γ$ and $e$ serve as inputs and $τ$ serves as either an input or output (a point that we revisit in detail in \autoref{sec:specifying-lambda-terms}).
To derive a type-aware term enumeration system from this judgment, let's simply flip the inputs and outputs: $Γ$ and $τ$ will be inputs and the output will be an $e$.

\input{figures/stlc-gen.tex}

\autoref{fig:stlc-gen} gives the rules for \stlc{} well-typed term enumeration, written $Γ ⊢ τ ⇝ e$.
A complete derivation in this judgment corresponds to how we generate a particular term $e$ of type $τ$.
Of note, rules \rulename{gen-var} and \rulename{gen-base} make non-deterministic choices.
\rulename{gen-var} chooses any variable of the appropriate type from the context.
\rulename{gen-base} chooses from among any of the constants of type $T$.
When we move towards creating an algorithm for term enumeration, we will need to resolve this non-determinism.

As a concrete example, we generate the well-typed closed term $(λx{:}T → T.\,x)\;(λx{:}T.\,c_1)$ through the following derivation:
\[
\inferrule*
  {\inferrule*
    {\inferrule*
      {x{:}T → T ∈ x{:}T → T}
      {x{:}T → T ⊢ T → T ⇝ x}}
    {· ⊢ (T → T) → (T → T) ⇝ (λx{:}T → T.\,x)} \\
  \inferrule*
    {\inferrule*
      { }
      {x{:}T ⊢ T ⇝ c_1}}
    {· ⊢ (T → T) ⇝ (λx{:}T.\,c_1)}
  }
  {· ⊢ T → T ⇝ (λx{:}T → T.\,x)\;(λx{:}T.\,c_1)}
\]
Note that the derivation for term enumeration is identical to the derivation for type checking!
This is because the rules for well-typed term enumeration mirror the rules for type checking.
This observation leads to two natural properties of our term enumeration judgment:

\begin{theorem}[Soundness of \stlc{} term enumeration]
\label{thm:soundness-stlc-term-enumeration}
If $Γ ⊢ τ ⇝ e$ then $Γ ⊢ e : τ$.
\end{theorem}
\begin{proof}
Proof by induction on the derivation $Γ ⊢ τ ⇝ e$.
\end{proof}

\begin{theorem}[Completeness of \stlc{} term enumeration]
\label{thm:completeness-stlc-term-enumeration}
If $Γ ⊢ e : τ$ then $Γ ⊢ τ ⇝ e$.
\end{theorem}
\begin{proof}
Proof by induction on the derivation $Γ ⊢ e : τ$.
\end{proof}

\autoref{thm:soundness-stlc-term-enumeration} states that enumerated terms are well-typed, and \autoref{thm:completeness-stlc-term-enumeration} states that we are able to enumerate the well-typed terms.
By combining both theorems, we know that well-typed term enumeration produces \emph{exactly} the set of well-typed terms, \ie, the ``All Typed'' dataset from \autoref{fig:lambda-terms-counts}.

\subsection{Enumerating Normal Forms}
\label{subsec:enumerating-normal-forms}

By restricting term generation to well-typed terms, we substantially reduce the search space of programs.
However there are still many redundant terms that we could generate.
For example, the following terms are redundant
\begin{itemize}
  \item $(λx{:}T.\,x)\;c_1$ and
  \item $(λx{:}T → T.\,x)\;(λy{:}T.\,y)$.
\end{itemize}
These terms evaluate, according to our call-by-value semantics, to the values $c_1$ and $λy{:}T.\,y$, respectively.
In general, any term that is not a value, \ie, not in \emph{$β$-normal form}, is redundant with the value that it evaluates to.
We say that the two terms are \emph{$β$-equivalent}, taking the standard definition of $β$-equivalence as the smallest equivalence relation that contains our evaluation relation ($⟶$).\footnote{%
  This is true of \stlc{} because it is strongly normalizing.
  In the presence of non-termination, terms with infinite loops do not have a corresponding normal form.
}

By restricting our search to these normal forms, we narrow the search space of programs even further!
Concretely, \autoref{fig:lambda-terms-counts} shows that number of closed normal forms of size 15 and type $T → T$ is another order of magnitude smaller than the number of terms of type $T → T$ (1,489 versus 6,205 such terms).
To do this, we restrict terms so that they cannot contain pending $β$-reductions.
In \stlc{}, the only $β$-reduction available is function application which applies whenever we have a term of the form $(λx{:}τ.\,e)\;v$.
We avoid this situation by splitting expressions into two sets of sub-terms.
\begin{align*}
  E &\bnfdef x \bnfalt E\;I \\
  I &\bnfdef E \bnfalt λx{:}τ.\,I \bnfalt c
\end{align*}
$E$-forms include variables and \emph{elimination} forms, and $I$-forms include \emph{introduction} forms.
For example, lambdas introduce values of arrow type and function application eliminates these values.
In contrast, the base type $T$ is introduced by its constants $c_1, …, c_k$ and have no corresponding elimination forms.

With this syntax, $I$ and $E$ terms are in normal form by construction!
To see this, note that function application requires that the head of a function application is an $E$, and the syntax of $E$s bottom outs at variables.
Therefore, all applications will be of the form $x\,I_1, …, I_m$ where a variable is always at the head of the application.
Arguments, in contrast, are allowed to be any $I$-term which includes $E$s because $E$s are included in the definition of $I$.
\todo{%
  Technically, $I$ terms are in full $β$-normal form where reductions are allowed under lambdas.
  In contrast, $e$ normal forms may contain reductions under binders.
  Since terms always terminate, this is not an issue as the only $e$s that do not have a corresponding $I$s are those that contain infinite loops under lambdas, \eg, $λx{:}τ.\,f\;x$ where $f\;x$ loops.
  However, to show that $I$s fully represent the set of $e$s, we need a result showing that for each $e$, we can produce a contextually equivalence $I$.%
}

This technique of splitting the syntax of expressions into introduction and elimination forms has several useful effects.
In proof search, this syntax is useful for generating normal form proofs which, by the Curry-Howard Isomorphism~\todo{cite}, is equivalent to generating normal form programs in our setting~\todo{cite}.
Dividing the syntax in this manner also facilities a description \emph{bidirectional type checking}~\todo{cite} where we explicitly state when a type is an input or output in the type checking judgment.
This seemingly pedantic distinction becomes highly useful when we talk about refining specification in the context of full program synthesis in \autoref{sec:introducing-lsyn}.

\input{figures/stlc-gen-normal.tex}

\autoref{fig:stlc-gen-normal} gives the rules for generating \stlc{} terms in this restricted term language.
Because there are two syntactic forms, we introduce two judgments to synthesize $E$-($\synE$) and $I$-($\synI$) terms.
Note that the judgments are mutually recursive because the definition of $E$ and $I$ terms is mutually recursive.
The rule \rulename{gen-Ielim} allows us to generate an $E$ where ever an $I$ is expected (because $E$s are a proper subset of the $I$s).
And the rule \rulename{gen-Eapp} generates the function as an $E$ and its argument as an $I$.
Otherwise, the rules are identical to the $e$-term generation judgment given in \autoref{fig:stlc-gen}.

\section{Specifying \texorpdfstring{$λ$}{λ}-terms}
\label{sec:specifying-lambda-terms}

With well-typed term enumeration, we can efficiently search the space of \stlc{} programs.
In addition to this search method, we also require some way of \emph{specifying} which particular programs we would like to find during the synthesis process.
Specification can take many forms, for example, logical properties~\todo{cite} or execution traces~\todo{cite}.
Here, we will consider specifying programs with a combination of \emph{types} and \emph{examples}, in particular, input/output pairs of the form $(v ⇒ v')$ that say a synthesized function should produce the value $v'$ when given the value $v$.

\subsection{Examples as Specification}
\label{subsec:examples-as-specification}

There are trade-offs involved in choosing examples as our means of specification over other options.
Examples usually act as incomplete specifications as a finite set of input/output pairs can only specify a finite subset of a function's behavior.
This is a significant limitation because usually many non-equivalent programs satisfy a set of examples, and the synthesizer must choose among them according to the inferred intent of the user.
For example, if the synthesizer is provided with the single input/output example
\[
  c_1 ⇒ c_2 ⇒ c_1
\]
that states the synthesized program should return $c_1$ when given $c_1$ and $c_2$ as input, it isn't clear if the user wanted the program that always selects its first argument
\[
  λx{:}T.\,λy{:}T.\,x,
\]
or the program that always returns $c_1$
\[
  λx{:}T.\,λy{:}T.\,c_1.
\]
The user must specify more examples which is burdensome, or the synthesizer must resort to some heuristics to choose a final program, \eg, picking the smallest program with the least amount of variables, which may not produce the desired program in all cases.
Thankfully, heuristics like this tend to work well in practice~\todo{cite}.
Furthermore, this ambiguity can be mitigated by smart interface design decisions, \eg, displaying the top five synthesis results rather than a singular result.~\todo{cite}.

On the other hand, examples are typically more natural to write down than complete logical statements.
In some cases these logical statements are as complex---or even more complex---then the programs we intend to synthesize!
For example, consider the set of input/output examples specifying a function in a language extended with natural numbers and lists:
\begin{align*}
  []  &⇒ [] \\
  [0] &⇒ [0] \\
  [0, 1] &⇒ [1, 0] \\
  [0, 1, 2] &⇒ [2, 1, 0]
\end{align*}
From this limited set of examples, the user's intent seems obvious: they want to synthesize a function that sorts a list of natural numbers.

In contrast, consider a logical specification of how this function ought to behave:
\begin{gather*}
  \mkwd{sorted}(l) = ∀ x :: l' ⊑ l.\; ∀ y ∈ l.\; x < y
\end{gather*}
The $\mkwd{sorted}$ predicate states that for all sub-lists ($⊑$) of $l$, the head $x$ of that sub-list is least element of that sub-list.
While the logical specification precisely describes the behavior of any function that sorts, it takes a bit of ingenuity to come up with this specification.

Finally and most importantly, we use examples because they decompose naturally with types.
For example, the input/output example $[0, 1] ⇒ [1, 0]$ decomposes according to the arrow type $[\mkwd{nat}] → [\mkwd{nat}]$.
We know that when the synthesized function's argument has the value $[0, 1]$, the body of that function must evaluate to $[1, 0]$.
In contrast, it is not clear how to decompose the $\mkwd{sorted}$ predicate according to the type of the function we are synthesizing.

\subsection{Example Specifications in \texorpdfstring{\stlc}{λ→}}
\label{subsec:example-specification-in-stlc}

To adopt example specifications into \stlc{}, we need to account for the presence of higher-order functions.
Therefore, we adopt the following grammar of \emph{example values} $χ$ that we use as our specification:
\[
  χ \bnfdef c \bnfalt \many{v_i ⇒ χ_i}{i < n}
\]
Intuitively, for each type of \stlc{}, we include a term that stands in as an example of that type.
In most cases, this term is simply the values for that type, \eg, the constants $c_k$ for base type $T$.
However, lambdas serve as a poor example value for arrow types because providing a function value is tantamount to describing exactly how the function ought to behave!
Instead, we make sets of input/output pairs first-class example values with \emph{partial function} terms written:
\[
  v_1 ⇒ χ_1 \bnfalt … \bnfalt v_n ⇒ χ_n
\]
A partial function introduces $n$ input/output pairs of the form $v_i ⇒ χ_i$ that specify that when the synthesized function is supplied a $v_i$, it produces a $χ_i$.
Like $→$, $⇒$ associates to the right, and as a result, functions of $m$ arguments are represented in curried style, $v_1 ⇒ … ⇒ v_m ⇒ χ$.
Because of this, we restrict the right-hand side of a $⇒$ to be an example value.
This ensures that lambdas never appear as the \emph{goal} of a synthesis problem.
In contrast, the left-hand side of a $⇒$ is a regular value which includes lambdas (and not partial functions).
This means that when we give examples for higher-order functions, we provide function values for the arguments of those functions.
For short-hand, we write the above partial function as $\many{v_i ⇒ χ_i}{i < n}$ where the partial function contains $n$ input/output pairs.

As a concrete example within \stlc{}, consider the following synonyms for the church encodings of booleans over base type $T$:
\begin{align*}
  \mkwd{Bool}  &≝ T → T → T   \\
  \mkwd{true}  &≝ λt{:}T.\,λf{:}T.\,t \\
  \mkwd{false} &≝ λt{:}T.\,λf{:}T.\,f
\end{align*}
Then the following partial function:
\begin{gather*}
  \mkwd{true} ⇒ c_1 ⇒ c_2 ⇒ c_1 \bnfalt \mkwd{false} ⇒ c_1 ⇒ c_2 ⇒ c_2
\end{gather*}
specifies the $\mkwd{if}$ function of type $\mkwd{Bool} → T → T → T$, usually defined as:
\[
  \mkwd{if} ≡ λb{:}\mkwd{Bool}.\,λt{:}T.\,λf{:}T.\,b\;t\;f
\]
Note that the convenience of specifying arguments to higher-order functions as function values makes the examples read naturally.

\section{The Synthesis Problem}
\label{sec:the-synthesis-problem}

Now that we have identified the sort of specification we will use during synthesis, we must describe \emph{how} we will use the specification during the synthesis process.
Our synthesis problem consists of a \emph{goal type} $τ$ and a collection of \emph{example values} $Χ = χ_1, …, χ_n$ of that type $τ$.
A solution to the synthesis problem is a program $e$ that satisfies the examples $X$.

What does it mean for a program to satisfy a set of examples?
Intuitively, if we think of 

A simple way to utilize specification is to simply enumerate terms as per \autoref{fig:stlc-gen-normal} and check each to see if they satisfy the example values.
With our simple grammar of example values, this is straightforward:
\begin{itemize}
  \item At $\mkwd{Unit}$ type, the example value must be a $()$.  Therefore, the term must evaluate to $()$.
  \item At arrow type, the example value must all be partial functions $\many{v_i ⇒ χ_i}{i < m}$.
    The term $f$ that we synthesize is a function, so it suffices to check for each alternative $v ⇒ χ$ of the partial function that $(f\,v)$ evaluates to $χ$.
\end{itemize}
However, this process doesn't utilize examples at all \emph{during} term generation.
This seems like a waste because we ought to be able to refine our search based off the example values that the user provides.

\section{Introducing \texorpdfstring{$λ_{syn}$}{λsyn}}
\label{sec:introducing-lsyn}

So far, we modified the type checking judgment of \stlc{} to create a type-directed term enumeration judgment and defined a grammar of example values to serve as our specification.
Let us combine these two into a \emph{synthesis calculus}, \lsyn{}, that synthesizes \stlc{} terms.

\input{figures/lsyn-defn.tex}

\autoref{fig:lsyn-defn} gives the syntax of \lsyn{} which features an external language of standard expressions $e$ taken from \stlc{} and an internal language that splits up expressions into introduction forms $I$ and elimination forms $E$.
The internal language is a subset of the external language; as described in \autoref{subsec:enumerating-normal-forms}, it is precisely the normal forms of the external language.

Type checking and evaluation in the external language is the same as \stlc{}.
We provide additional rules for type checking the additional constructs introduced in \lsyn{}.
In particular, $E$ and $I$ terms are checked using bidirectional type checking~\todo{cite}.
We note that during regular type checking, $E$ terms \emph{generate} types (as an output), and $I$ terms \emph{check} against types (as an input).
To make this explicit, we separate $E$ and $I$ type checking into two separate judgments were we generate types $(E ⇒ τ)$ in the former case and check against types in $(τ ⇐ I)$ in the latter case.
For example when type checking a variable (\rulename{t-Evar}), we can extract the type of that variable from the context.
In contrast, when type checking a function (\rulename{t-Ilam}), while we know the type of the argument from the ascription, we have no information about the body, so we must check the body against a given input type.

Type checking examples values $χ$, in contrast, is straightforward.
The unit example value $()$ has type $\mkwd{Unit}$ (\rulename{t-ex-unit}).
A partial function $\many{v_i ⇒ χ_i}{i < m}$ has type $τ_1 → τ_2$ if all of $v_i$ have type $τ_1$ and all of the $χ_i$ have type $τ_2$.
When synthesizing a program, we give both a set of example values and a goal type $τ$ with the assumption that each example value has type $τ$.

The user does not provide a single example, but sets of examples $Χ$.


\subsection{Example Refinement}
\label{subsec:example-refinement}

While we have defined \emph{what} our specification is---input/output example values $χ$---we have not yet described \emph{how} to use this specification to refine our search.
A simple way to utilize specification is to simply enumerate terms and check each to see if they satisfy the example values.
With our simple grammar of example values, this is straightforward:
\begin{itemize}
  \item At $\mkwd{Unit}$ type, the example value must be a $()$.  Therefore, the term must evaluate to $()$.
  \item At arrow type, the example value must all be partial functions $\many{v_i ⇒ χ_i}{i < m}$.
    The term $f$ that we synthesize is a function, so it suffices to check for each alternative $v ⇒ χ$ of the partial function that $(f\,v)$ evaluates to $χ$.
\end{itemize}
However, this process doesn't utilize examples at all \emph{during} term generation.
This seems like a waste because we ought to be able to refine our search based off the example values that the user provides.

\input{figures/lsyn-synthesis.tex}

To fix this problem, we introduce \emph{example refinement} to create a full-fledged synthesis procedure within \lsyn{}.
We break up synthesis into two judgments as shown in \autoref{fig:lsyn-synthesis}:
\begin{itemize}
  \item $Γ ⊢ τ ⇝ E$ (\rulename{eguess}): guess an $E$ of type $τ$.
  \item $Γ ⊢ τ ▷ Χ ⇝ I$ (\rulename{irefine}): refine and synthesize an $I$ of type $τ$ that agrees with examples $Χ$.
\end{itemize}

With $I$-terms, we are able to leverage the examples the user provides.
For example, at $\mkwd{Unit}$ type, there is only one value, $()$ of that type, so the examples values must all be $()$.
