Type systems are the most ubiquitous form of formal verification tool present in programming languages today.
They provide a number of important benefits for programmers, for example, static checking of errors and opportunities for optimizations.
However, one of the most often overlooked benefits is that type systems also help programmers design programs more efficiently.
In particular, typed, functional programming languages like ML and Haskell provide a compelling combination of rich types coupled with a succinct, yet powerful set of languages features.
Programmers that use these languages frequently comment that once they figure out the types of their program, the program just writes itself!

Let's investigate this point in more detail.
Consider writing a simple program, say the $\mkwd{map}$ function over lists, in a ML-like language.
First, let's establish the type of the overall function: $\mkwd{map}$ is a higher-order function that takes (1) a function that transforms a single element of a list into some other type, (2) a list of the first type, and produces a list of the second type.
This means that $\mkwd{map}$ has type $({'a} → {'b}) → {'a}\,\mlist → {'b}\,\mlist$ where ${'a}\,\mlist$ is ML's way of writing down a \emph{polymorphic} or \emph{generic} list.
Here, ${'a}$ is a type variable that represents the type of elements of that list, its \emph{carrier type}.
Now, let's develop this function incrementally in a \emph{type-directed} manner by keeping close track of the types of expressions we need to fill in at each point of the program.
Initially, we have the following goal,
\[
  ◼ : ({'a} → {'b}) → {'a}\,\mlist → {'b}\,\mlist,
\]
where $◼$ represents our \emph{goal}---the hole in the program we need to fill with an expression.
Because the hole is at arrow type, it makes sense to introduce a function:
\[
  \mlet\;\mrec\;\mkwd{map}\;(f{:}{'a} → {'b}) (l{:}{'a}\,\mlist) : {'b}\,\mlist = ◼ : {'b}\,\mlist.
\]

With the function written down, our goal is to now fill in the body of the function which, by the definition of the function, must produce a ${'b}\,\mlist$.
Note how the rich types greatly constrain the set of programs we can write at this point.
Because the carrier type of the output list is unknown, we have no way of constructing a non-empty ${'b}\,\mlist$.
The only other value we can provide is the empty list which means $\mkwd{map}$ would produce the empty list for any pair of inputs which is not correct.
We need more information to make progress, and we obtain that information by \emph{pattern matching}:
\[
  \begin{array}{l}
    \mlet\;\mrec\;\mkwd{map}\;(f{:}{'a} → {'b}) (l{:}{'a}\,\mlist) : {'b}\,\mlist = \\
      \⇥{1}\mmatch\;l\;\mwith \\
      \⇥{1}\bnfalt \mNil → ◼ : {'b}\,\mlist \\
      \⇥{1}\bnfalt \mCons(x, {l'}) → ◼ : {'b}\,\mlist.
  \end{array}
\]
By pattern matching, we perform case analysis on some value.
In this case, we know that a list in ML has two possible \emph{constructors} or ways of making a value of type $\mlist$: $\mNil$ representing the empty list and $\mCons$ representing the list composed of a single element $x$ followed by the rest of the list ${l'}$.
We are left with two goal expressions to fill in, both of type ${'b}\,\mlist$, corresponding to the branches of the pattern match.

In the $\mNil$ branch, we know the input $l$ is the empty $\mlist$ at this point in the program, so it is sensible to produce the empty list.
In the $\mCons$ branch, we know that the $\mlist$ has at least one element $x$ of type ${'a}\,\mlist$ and we have a handle on the rest of the list ${l'}$.
With $x$, we now have a way of manufacturing a ${'b}\,\mlist$: applying $x$ to $f$.
With this, we can transform the head of list $l$ and the cons that onto the result of recursively mapping over the tail of $l$.
Thus, the final program is:
\[
  \begin{array}{l}
    \mlet\;\mrec\;\mkwd{length}\;(l{:}\mlist) : \mnat = \\
      \⇥{1}\mmatch\;l\;\mwith \\
      \⇥{1}\bnfalt \mNil → \mNil \\
      \⇥{1}\bnfalt \mCons(x, {l'}) → \mCons(f\,x, \mkwd{map}\,{l'}).
  \end{array}
\]
At the end, we needed some ingenuity to recognize how to break down the mapping operation over a list in terms of its components.
However, we can see how types guided our development process by constraining the allowable set of programs at different points of the program.
The question is simple: can we automate this sort of type-directed reasoning?
Given a type, and perhaps some additional specification of how a program should behave, can we derive a program of the appropriate type that meets this specification?

This process of generating programs automatically from specification is called \emph{program synthesis} and is one of the greatest, longest-standing pursuits of computer science.
At its core, program synthesis is a problem of combining \emph{search} with \emph{specification}.
The domain of the search is the infinite sea of possible programs (for some particular programming language).
Specification allows us to pick out particular programs of interest that we find during search.
This specification can take many forms, for example, logical statements~\citep{green-ijcai-1969, manna-tse-1979}, input/output examples~\citep{summers-popl-1976, kitzelmann-thesis-2010, albarghouthi-cav-2013, feser-pldi-2015}, and partial programs~\citep{solar-lezama-thesis-2008, alur-fmcad-2013, singh-pldi-2013}, among others.
Regardless of the mode of specification we choose, the search and specification components of synthesis frequently inform each other; specification helps refine the space of possible programs, making search tractable, and different search techniques are more amendable to particular kinds of specification.

As you can imagine, program synthesis is a very difficult problem---we are still writing programs by hand, after all---and undecidable in the general case.
However, it is worth pursuing because the benefits of program synthesis technology are immense.
By synthesizing a program from a specification, we guarantee that the program agrees with that specification by virtue of the synthesis process.
If that specification includes properties such as correctness or safety, then the program will enjoy those properties automatically.
Furthermore, it is often easier to write down a specification of how a program should work than to write the program itself, especially if that specification is partial, for example, a collection of input/output examples or a demonstration of how the program should work.
Program synthesis then becomes a tool that makes the power of computer programs more accessible to people, especially non-programmers, who can signify their intent, but do not know how to translate that intent into a program.
Finally, because computer programs are general-purpose, synthesizing programs means synthesizing methods of solving any task we can express as a program.
This might mean deriving a complex program to determine the trajectory of a rocket or simply automating the task of entering data into a spreadsheet~\citep{gulwani-popl-2011}.

In this work, we develop a \emph{type-theoretic interpretation} of program synthesis.
Type theory~\citep{martin-lof-1984} provides a constructive foundation for all of mathematics.
Programming language researchers leverage these foundations through the Curry-Howard Isomorphism~\citep{william-curry-1980} which equates proofs of propositions with programs of some type.
By studying program synthesis through the lens of type theory, we hope to bring to bear the large body of work on type systems and proof search onto the program synthesis problem to enable efficient synthesis of typed, functional programs.

\section{The Landscape of Program Synthesis}
\label{sec:the-landscape-of-program-synthesis}

Many sub-disciplines within computer science tackle the problem of search in different ways, and as a result, these disciplines all have unique perspectives on program synthesis.
The earliest such efforts came out of artificial intelligence and automatic theorem proving communities during the 60s and 70s~\citep{green-ijcai-1969, summers-popl-1976}.
Since then approaches from machine learning~\citep{lau-thesis-2001, briggs-kes-2008, weimer-icse-2009}, formal methods~\citep{srivastava-popl-2010, bodik-popl-2010, kuncak-pldi-2010}, and programming language theory~\citep{albarghouthi-cav-2013, gvero-pldi-2013, scherer-icfp-2015} have all been applied to the program synthesis.
Today, the field has seen a large resurgence in interest due to a number of factors:
\begin{enumerate}
  \item General computational power has increased at an exponential rate over the last four decades.~\citep{moore-electronics-1965}
  \item The rise of domain-specific languages, and more targeted programming domains such as protocols~\citep{alur-popl-2005, udupa-pldi-2013}, concurrent programs~\citep{solar-lezama-pldi-2008, cerny-cav-2011, prountzos-oopsla-2012}, education~\citep{singh-pldi-2013}, and strings and spreadsheets~\citep{gulwani-popl-2011} has given synthesis tools smaller, more tractable domains to operate over.
  \item Related to the first point, the rise of sophisticated solver technology, in particular SAT and SMT solvers~\citep{barrett-smt-2008} have helped make once intractable problems of search more feasible in practice.
\end{enumerate}
Here we briefly survey the field of program synthesis\footnote{%
  Because program synthesis is such a vast field of study, we don't intend on capturing its full breadth here.
  For more thorough introductions to program synthesis, we recommend reading the surveys by \citet{kreitz-automated-deduction-1998}, \citet{flener-jlp-1999}, \citet{gulwani-ppdp-2010}, and \citet{kitzelmann-aaip-2010}.
} to get a sense of what approaches have been previously studied and how they contrast with our own type-theoretic style.
We defer discussion of how these approaches contrast with our own until after we present the details of how our type-directed program synthesis systems operate in the presence of various types (\autoref{sec:related-work-lsyn} and \autoref{sec:related-work-mlsyn}).

\subsection{Methodologies}
\label{subsec:methodologies}

\paragraph{AI and Logic-based Techniques}

The earliest methods for program synthesis were developed from the automated theorem proving community.
These researchers were motivated by the promise of generating programs from specifications that were provably correct by virtue of being derived from specification.
Some methods used techniques lifted from early automated theorem prover technology, such as Green's resolution-based \textsc{QA3} system, to translate programs from logical specification~\citep{green-ijcai-1969}.
These specifications took the form of complete axiomitizations of the problem space in first-order logic solved using resolution-based proof search techniques.
Others took to direct rewriting tactics over the specification, for example, Manna and Waldinger's \textsc{Deadalus} system which rewrote logical specification~\citep{manna-tse-1979} and Summers's \textsc{Thesys} system which rewrote examples~\citep{summers-popl-1976}.
Later, efforts from researchers such as Manna and Waldinger sought to unify the use of provers and transformation rules~\citep{manna-plas-1980}.
Because all of these early works were rooted within theorem proving, the target language for synthesis was universally (a purely functional subset of) Lisp.

These methods were technically innovative, but ultimately lacked in practicality.
In particular, the logical specifications demanded by these tools were far removed from the reasoning styles that programmers understood.
They were also highly constrained in the sorts of program symbols they were allowed to utilize.
Finally, they were extremely heavyweight and did not scale past anything but the smallest of example programs~\citep{kreitz-automated-deduction-1998}.

Regardless, \textsc{Thesys} is particularly note worthy in that it is one of the first \emph{inductive programming} synthesis systems~\citep{kitzelmann-aaip-2010} where it is able to generate programs in the presence of \emph{partial specification} such as examples.
Examples form a partial specification because most programs of interest have an infinite range and a finite set of concrete examples can only specify a finite subset of that range.
More modern systems such as \textsc{Igor2} have evolved from this line of work, and have overcome many of issues listed above~\citep{kitzelmann-thesis-2010, hofmann-aaip-2010}.
\textsc{Igor2} uses a combination of examples, background knowledge, and program schemes---program skeletons that capture recurring patterns of program behavior such as folds or maps---to derive target functions by discovering patterns in the examples and deriving a set of recursive rules for generating them.

In contrast, other inductive programming systems do not perform manipulation of the examples directly.
Rather they employ a \emph{guess-and-check} approach where they enumerate candidate programs and evaluate them to verify that they satisfy the examples.
For example Katayama's \textsc{MagicHaskeller}~\citep{katayama-pepm-2012} enumerates programs according to a set of
logical rules and permitted components and evaluates them against user-provided input/output examples.
And Albarghouthi's \textsc{Escher}~\citep{albarghouthi-cav-2013} builds up increasingly complicated components from atoms (\ie, variables and constants) and tests whether those atoms satisfy the examples.
Notably, when the system requires additional examples, such as when it evaluates a recursive function call, \textsc{Escher} queries the user to provide additional examples.
\textsc{LaSy}~\citep{perelman-pldi-2014} provides an example-driven framework for synthesizing programs in expert-written domain-specific languages.
Finally, Feser's $λ^2$ system~\citep{feser-pldi-2015} also enumerates programs and checks them against examples.
However, unlike previous efforts, they \emph{refine} examples as they synthesize expressions, producing new examples appropriate for synthesizing the sub-expressions of this overall expression.
Notably, these final approaches blur the line between the AI/logical tradition of program synthesis born from the original literature from the 70s and the more modern verification-based tradition that we see today.\footnote{%
  At least, ``modern'' by the standards of when this thesis is written.
  While program synthesis has proven to be an enduring problem, the different approaches understandably come and go as computer science matures and our technology becomes more advanced.
}

Note that while not directly related to program synthesis, because they explore the search space of programs through term enumeration, these guess-and-check inductive programming approaches share many concerns with \emph{automatic test generation}~\citep{claessen-flp-2014, grygiel-jfp-2013, yakushev-aaip-2010}.
In particular when enumerating terms, we want to avoid generating redundant or otherwise unnecessary terms, in particular, terms that are equivalent to previously generated terms.

\paragraph{Machine Learning Techniques}

Bridging the gap between logic and machine learning are \emph{inductive logic programming} (ILP)~\citep{muggleton-jlp-1994} techniques that apply machine learning to problems expressed in first-order logic, \ie, Prolog programs.
Inductive logic programming is an umbrella term representing an entire sub-field of machine learning that employs this methodology for solving learning-based problems.Researchers that study inductive logic programming synthesis~\citep{flener-jlp-1999} apply these techniques specifically to program synthesis.
For example, \citet{sankaranarayanan-icse-2008} use ILP to mine library specifications by running unit tests on a library to gather information about the operations of the library.
This information is then processed using ILP to produce Prolog specifications of the library's behavior.

Other approaches rooted in machine learning have been applied to program synthesis as well.
For example, genetic programming techniques have been used by \citet{briggs-kes-2008} to synthesize combinator expressions in a typed, functional programming language and by \citet{weimer-icse-2009} to automatically locate bugs and derive patches in legacy C code.
\citet{gulwani-popl-2007} used probabilistic inference to synthesize imperative programs from input/output examples.
And finally, \citet{lau-thesis-2001} in her thesis developed version space algebras to synthesize text editor macros from examples---here, demonstrations by the user of the text macro they intended for the system to synthesize.

\paragraph{Verification-based Techniques}

The most recent efforts in program synthesis lie in the programming languages community.
In particular, as off-the-shelf SAT and SMT solver technology like the Z3 theorem prover~\citep{demoura-tacas-2008} rapidly matured over the last decade, the verification community has been quick to take advantage of their power.
With respect to program synthesis, this means transforming the specification given by the user into a series of constraints that can be discharged by the solver.
The output of the solver can then be used to guide the search process accordingly.

The most well-known use of solver technology in program synthesis is Solar-Lezama's \textsc{Sketch}~\citep{solar-lezama-thesis-2008}.
Sketch allows users to write skeletons of Java-like programs annotated with holes whose contents are specified by generator expressions that describe the allowable set of program constructs for those holes.
Sketch then translates the constraints on those holes, \eg, assertions or reference implementations, into satisfiability equations which are then discharged by a SMT solver using Counterexample Guided Inductive Synthesis (CEGIS).
Other work that uses solver technology in synthesis includes \citet{bodik-popl-2010}'s work to support incremental program develop with holes and examples, called \emph{angelic nondeterminism} and Torlak's \textsc{Rosette} which supports the development of solver-aided domain specific languages~\citep{torlak-pldi-2014}.

In many of these situations, we can refine the problem domain sufficiently that we can restrict the syntax of allowed programs to a small subset, an approach called \emph{syntax-guided synthesis}~\citep{alur-fmcad-2013}.
For example, \citet{singh-pldi-2013} use this approach in the context of generating automatic feedback for introductory programs.
The restricted syntax, provided by an instructor using their system, captures the likely set of mistakes that a student might make on an assignment.
\citet{gulwani-popl-2011} also use a restricted synthesis domain to capture string processing behavior.
This synthesis technology is used, in turn, to implement the \textsc{FlashFill} feature of Microsoft Excel.

In addition to satisfiability solvers, other verification technology has been re-appropriated for the purposes of program synthesis.
In particular, techniques that leverage types, the focus of this dissertation, have been explored to some degree.
For example \textsc{Djinn}~\citep{augustsson-2004} synthesizes Haskell programs from highly refined type signatures.
\textsc{Prospector}~\citep{mandelin-pldi-2005}, Perelman's auto-completion tool for C\# programs~\citep{perelman-pldi-2012}, and \textsc{InSynth}~\citep{gvero-pldi-2013} all leverage types to accomplish code auto-completion.
Most recently, $λ^2$~\citep{feser-pldi-2015} uses types to refine the input/output examples that they receive, and \citet{scherer-icfp-2015} phrase program synthesis in terms of type inhabitation.

\section{Program Synthesis With Types}

From \autoref{sec:the-landscape-of-program-synthesis}, we see that a multitude of approaches to program synthesis have been previously explored, each with their own strengths and weaknesses.
However, no single system has the combination of features necessary to fully capture the type-directed programming style that motivated our journey into program synthesis to begin with.
In particular, this style requires support for higher-order functions, recursive functions, and algebraic data types.
Many of the prior systems focus on languages that do not support one of more of these features as they are based on variants of Lisp, C, or Java.
Furthermore, many of the systems that use solver technology are not capable of handling higher-order and/or recursive data as these solvers work over first-order logics.
Of the systems that most closely target this space of language features:
\begin{itemize}
  \item \textsc{Escher}~\citep{albarghouthi-cav-2013} synthesizes recursive functions using input/output examples, but in a Lisp-like untyped setting.
  \item \textsc{Leon}~\citep{kuncak-pldi-2010} and \textsc{Igor2}~\citep{hofmann-aaip-2010} synthesize recursive functions using input/output examples over algebraic data types, but they cannot handle higher-order functions.
    In particular, \textsc{Leon}'s reliance on solver technology keeps them from handling higher-order functions.
    \textsc{Igor2} allows for usage of higher-order function components but it does not appear to synthesize functions that take higher-order functions as arguments.
  \item $λ^2$~\citep{feser-pldi-2015} synthesizes recursive functions over algebraic data types using input/output examples.
    However, is is entirely component driven---$λ^2$ only synthesizes the composition of function applications efficiently---and cannot pattern match over algebraic data types.
\end{itemize}
Furthermore, it is not clear whether any of these systems scale up to richer type systems such as linear types~\citep{girard-1987}, refinement types~\citep{freeman-pldi-1991}, and dependent types~\citep{martin-lof-1984}.

\subsection{Foundations}
To address these concerns, we create a theoretical foundation for program synthesis using types.
With this foundation, we answer several key questions:
\begin{itemize}
  \item How can we take advantage of rich types in order to prune the search space of possible programs?
  \item How can we use types to incremental refine the specification provided by the user in step with the program that we synthesize?
  \item What are the meta-theoretic properties of synthesis systems based on types?  In particular, are they sound and complete?
\end{itemize}
The basis of our foundation is a technique for transforming a programming language's type system into a type-directed, example-powered program synthesis system for that language.
This transformation allows us to gain immediate insight into how to synthesize programs and refine examples of particular types.
In some cases, this insight alone is sufficient to integrate a new type into our synthesis systems; in others, we must address additional issues that arise when synthesizing programs of these types.

We begin by applying this technique to the simplest type system possible, the simply-typed lambda calculus (\autoref{ch:a-simple-synthesis-calculus}).
The resulting \emph{synthesis calculus}, \lsyn{}, allows us to explore in detail the transformation process as well as how these resulting synthesis systems operate.
We then begin integrating additional types into the mix with the goal of arriving at a synthesis system for a more realistic typed, functional programming language.
First, we consider simple extensions to the simply-typed lambda calculus---products, records, and sums (\autoref{ch:simple-type-extensions}).
And then, we build more complex synthesis calculi to handle complex type extensions---\mlsyn{} to handle recursion with algebraic data types (\autoref{ch:recursion}) and \systemfsyn{} to handle polymorphism (\autoref{ch:polymorphism}).

\subsection{Metatheory}
We use our core calculi---\lsyn{}, \mlsyn{}, and \systemfsyn{}---to study the meta-theoretic properties of program synthesis with types.
In particular, we are concerned with two key properties of synthesis systems:
\begin{itemize}
  \item \textbf{Soundness}: does the synthesized program obey our input specification?
  \item \textbf{Completeness}: can we synthesize all programs?
\end{itemize}
We first study the soundness and completeness of \lsyn{} in full detail (\autoref{ch:metatheory-of-lsyn}).
By doing so, we extract the key lemmas that we must prove to show that soundness and completeness holds of the whole synthesis calculus.
We then prove these lemmas for our simple extensions to \lsyn{} (\autoref{ch:simple-type-extensions}) as well as polymorphism (\autoref{ch:polymorphism}).
\mlsyn{} proves to be much more complex due to recursion, so we also investigate its metatheory in full detail (\autoref{ch:metatheory-of-mlsyn}).

\subsection{Implementation}
The synthesis calculi we develop in our work give us a basic understanding of the design and behavior of type-directed synthesis systems, but they are not practical synthesis algorithms as-is because they are both unoptimized and highly non-deterministic.
Consequently, in addition to understanding type-directed program synthesis from a theoretical perspective, we would like to know empirically how these systems behave on real-world examples.

To do this, we take the calculus closest to a real-world typed, functional programming language, \mlsyn{}, and transform it into an actual program synthesizer, \myth{}, for a core subset of the OCaml programming language~(\autoref{ch:implementation}).
Because our program synthesizer is type-theoretic, we are able to adapt several proof search techniques for our domain---various caching schemes and search pruning heuristics---to greatly optimize the synthesis procedure.
Furthermore, thanks to our theoretical foundations, we are able to analyze the impact of these optimizations on the soundness and completeness of our system.
Finally, we evaluate \myth{}'s effectiveness on a benchmark suite of functional programs and explore \myth{}'s behavior on these examples~(\autoref{ch:evaluating-myth}).

\subsection{Statement of Contribution}

The synthesis calculi for ML-like programs, \mlsyn{}, that we discuss in \autoref{ch:recursion} as well as the implementation, \myth{}, that we develop in \autoref{ch:implementation} and evaluate in \autoref{ch:evaluating-myth} were originally presented in {PLDI} 2015~\citep{osera-pldi-2015}.
The chapters listed above constitute a greatly expanded presentation of these two artifacts.
The presentation of tuples and records in \autoref{ch:simple-type-extensions} in our simply-typed synthesis calculus, \lsyn{}, was adapted from \citet{frankle-mastersthesis-2015} and \citet{shah-mastersthesis-2015}, respectively, who originally integrated these features into \mlsyn{}.
The remaining content is original work.
